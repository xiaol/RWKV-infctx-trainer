{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 2 & 3 Validation\n",
    "This model being trained has the same settings as raven 1B5 model.\n",
    "- Layer count: 24\n",
    "- Embed size: 2048\n",
    "\n",
    "The goal is to validate the trainer across deepspeed 2 & 3 - with and without offload. All other training params remain constant.\n",
    "\n",
    "Note, you will need a dual+ GPU setup, that is capable of handling deepspeed 2 (minimum 24GB * 2)\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps\n",
    ">\n",
    "> And that you have completed the `baseline-setup.ipynb` and the environment setup found in project `README.md`\n",
    "\n",
    "## What does deepspeed 2 & 3 do (With/Without CPU offload) ??\n",
    "\n",
    "Instead of simply splitting the dataset being trained, and having a full copy of nearly everything in all GPU's (aka DDP / DeepSpeed 1).\n",
    "\n",
    "Deepspeed 2, keeps a full copy of the model weights on each GPU, but splits the training gradient descent memory usage into multiple GPUs, or offload it into CPU memory (+ CPU offload option).\n",
    "\n",
    "Deepspeed 3, takes it a step further, and distributes the model weights across all the GPUs, drastically lowering the vram requirement, while increasing the amount of GPU to GPU traffic drastically. Gradient descent memory is still split across multiple GPUs, with the option to offload into CPU memory (Same as deepspeed 2)\n",
    "\n",
    "Finally, Deepspeed 3, also introduce options to further offload such model weights / gradient descent, more into CPU memory or NVMe. However this option was not enabled or explored in the following benchmarks.\n",
    "\n",
    "See more here: https://huggingface.co/docs/transformers/main_classes/deepspeed\n",
    "\n",
    "## Benchmark results\n",
    "\n",
    "Benchmark was done on 11th July 2023. With Torch 2.0.1, Cuda 11.8. On two seperate machines, via vast.ai\n",
    "\n",
    "- 2 x A5000 (AMD EPYC 7513 - per gpu specs: 24gb, pcie4x16, 34.4 TFlops, nvlinked)\n",
    "- 2 x 3090 (AMD EPYC 7302 - per gpu specs: 24gb, pcie3x16, 44.1 TFlops)\n",
    "\n",
    "| Deepspeed Strat       | Time (A5000)          | Time (3090)           | VRAM Usage       | RAM Usage | Validation Loss |\n",
    "| --------------------- | --------------------- | --------------------- | ---------------- | --------- | --------------- |\n",
    "| Stage 2               | 24 mins : 55 sec      | 35 mins : 04 sec      | ~22.3 + 23.8 GB  | ~85 GB    | 6.173           |\n",
    "| Stage 2 + CPU offload | 43 mins : 08 sec      | 59 mins : 04 sec      | ~9.7 + 10.3 GB   | ~128 GB   | 6.124           |\n",
    "| Stage 3               | 29 mins : 12 sec      | 50 mins : 04 sec      | ~23.0 + 23.2 GB^ | ~85 GB    | 5.665           |\n",
    "| Stage 3 + CPU offload | 1hr : 42mins : 38 sec | 1hr : 29mins : 15 sec | ~7.0 + 7.3 GB    | ~145 GB   | 5.668           |\n",
    "\n",
    "---\n",
    "\n",
    "> ^ note in theory deepspeed 3 uses less vram then deepspeed 2, however it will also try to use up more ram then its needed for \"cache\" items if possible, maxing out to the same level as deepspeed 2 here\n",
    "\n",
    "> Torch.JIT was enabled for deepspeed 2, But was disabled for deepspeed 3 (not compatible). Torch.compile was disabled\n",
    "\n",
    "> Full report with charts and figures can be found at WANDB : https://wandb.ai/picocreator/RWKV-InfCtx-Validation/reports/-RWKV-infctx-DeepSpeed-2-3-comparisons--Vmlldzo0ODM5MjAy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and apply your preferred settings\n",
    "\n",
    "Adjust your desired deepspeed settings, and gpu device count.\n",
    "\n",
    "Enable/Disable WANDB here as well ( Enabled by default, as we need the loss curve for this experiment )\n",
    "\n",
    "( note you will need to rerun this cell, if you restart your env )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"infctx-deepspeed-deterministic\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 2\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230710_201455-u3419fsk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-deepspeed-deterministic (deepspeed_stage_2, train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/u3419fsk\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 706.83it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-d9ebc51d67d5ce31_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c63aa2ba2a5c9680_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-193f68ea4e8054de_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-07-10 20:15:11,785] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-07-10 20:15:27,547] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-10 20:15:27,587] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.08059811592102051 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1025228500366211 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0865182876586914 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10229158401489258 seconds\n",
      "Rank: 0 partition count [2, 2, 2] and sizes[(757504000, False), (24576, False), (24576, False)] \n",
      "Rank: 1 partition count [2, 2, 2] and sizes[(757504000, False), (24576, False), (24576, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00031065940856933594 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005862712860107422 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [34:58<00:00,  1.26it/s, v_num=9fsk, train/loss=9.000\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|‚ñã                  | 1/27 [00:00<00:07,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|‚ñà‚ñç                 | 2/27 [00:00<00:06,  4.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|‚ñà‚ñà                 | 3/27 [00:00<00:05,  4.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|‚ñà‚ñà‚ñä                | 4/27 [00:00<00:04,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|‚ñà‚ñà‚ñà‚ñå               | 5/27 [00:01<00:04,  4.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|‚ñà‚ñà‚ñà‚ñà‚ñè              | 6/27 [00:01<00:04,  4.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|‚ñà‚ñà‚ñà‚ñà‚ñâ              | 7/27 [00:01<00:04,  4.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 8/27 [00:01<00:03,  4.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 9/27 [00:01<00:03,  4.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 10/27 [00:02<00:03,  4.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 11/27 [00:02<00:03,  4.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 12/27 [00:02<00:02,  5.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 13/27 [00:02<00:02,  5.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 14/27 [00:02<00:02,  5.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 15/27 [00:02<00:02,  5.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 16/27 [00:03<00:02,  5.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 17/27 [00:03<00:01,  5.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 18/27 [00:03<00:01,  5.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 19/27 [00:03<00:01,  5.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 20/27 [00:03<00:01,  5.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 21/27 [00:04<00:01,  5.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 22/27 [00:04<00:00,  5.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 23/27 [00:04<00:00,  5.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/27 [00:04<00:00,  5.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 25/27 [00:04<00:00,  5.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/27 [00:04<00:00,  5.30it/s]\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [35:04<00:00,  1.26it/s, v_num=9fsk, train/loss=9.000\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [35:04<00:00,  1.26it/s, v_num=9fsk, train/loss=9.000`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [35:04<00:00,  1.26it/s, v_num=9fsk, train/loss=9.000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 8.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 8.61401\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33minfctx-deepspeed-deterministic (deepspeed_stage_2, train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/u3419fsk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230710_201455-u3419fsk/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ../../RWKV-v4neo && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c ../notebook/trainer-validation/config/baseline-1024.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_2, train-ctx=1024, data-ctx=1024)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_2\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 2 + Offload\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230710_205104-ral9a67a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-deepspeed-deterministic (deepspeed_stage_2_offload, train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/ral9a67a\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 732.63it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-d9ebc51d67d5ce31_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c63aa2ba2a5c9680_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-193f68ea4e8054de_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-07-10 20:51:20,747] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-07-10 20:51:36,365] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-10 20:51:36,387] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: it is highly recommended to enable bptt_learning when used to deepspeed 2/3/offloading, otherwise an exception will occur when training with dataset records, larger then the configured context length (1024)\n",
      "[WARNING]: it is highly recommended to enable bptt_learning when used to deepspeed 2/3/offloading, otherwise an exception will occur when training with dataset records, larger then the configured context length (1024)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.370939254760742 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.391589403152466 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.08360075950622559 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10255146026611328 seconds\n",
      "Rank: 0 partition count [2, 2, 2] and sizes[(757504000, False), (24576, False), (24576, False)] \n",
      "Rank: 1 partition count [2, 2, 2] and sizes[(757504000, False), (24576, False), (24576, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004904270172119141 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005400180816650391 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [59:04<00:00,  1.34s/it, v_num=a67a, train/loss=6.560\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|‚ñã                  | 1/27 [00:00<00:05,  4.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|‚ñà‚ñç                 | 2/27 [00:00<00:05,  4.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|‚ñà‚ñà                 | 3/27 [00:00<00:04,  5.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|‚ñà‚ñà‚ñä                | 4/27 [00:00<00:04,  5.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|‚ñà‚ñà‚ñà‚ñå               | 5/27 [00:00<00:04,  5.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|‚ñà‚ñà‚ñà‚ñà‚ñè              | 6/27 [00:01<00:04,  5.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|‚ñà‚ñà‚ñà‚ñà‚ñâ              | 7/27 [00:01<00:03,  5.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 8/27 [00:01<00:03,  5.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 9/27 [00:01<00:03,  5.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 10/27 [00:01<00:03,  5.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 11/27 [00:02<00:02,  5.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 12/27 [00:02<00:02,  5.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 13/27 [00:02<00:02,  5.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 14/27 [00:02<00:02,  5.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 15/27 [00:02<00:02,  5.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 16/27 [00:02<00:02,  5.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 17/27 [00:03<00:01,  5.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 18/27 [00:03<00:01,  5.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 19/27 [00:03<00:01,  5.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 20/27 [00:03<00:01,  5.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 21/27 [00:03<00:01,  5.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 22/27 [00:03<00:00,  5.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 23/27 [00:04<00:00,  5.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/27 [00:04<00:00,  5.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 25/27 [00:04<00:00,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/27 [00:04<00:00,  5.56it/s]\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [59:13<00:00,  1.34s/it, v_num=a67a, train/loss=6.560\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [59:13<00:00,  1.34s/it, v_num=a67a, train/loss=6.560`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [59:13<00:00,  1.34s/it, v_num=a67a, train/loss=6.560\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 6.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 6.11343\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33minfctx-deepspeed-deterministic (deepspeed_stage_2_offload, train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/ral9a67a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230710_205104-ral9a67a/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ../../RWKV-v4neo && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c ../notebook/trainer-validation/config/baseline-1024.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_2_offload, train-ctx=1024, data-ctx=1024)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_2_offload\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 3\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230710_215131-7jwlhk35\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-deepspeed-deterministic (deepspeed_stage_3, train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/7jwlhk35\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 756.00it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-d9ebc51d67d5ce31_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c63aa2ba2a5c9680_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-193f68ea4e8054de_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-07-10 21:51:46,853] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-07-10 21:52:02,253] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-10 21:52:02,275] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.08010029792785645 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1015465259552002 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.08151650428771973 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0818178653717041 seconds\n",
      "Parameter Offload: Total persistent parameters: 548864 in 268 params\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00032210350036621094 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006728172302246094 seconds\n",
      "\n",
      "  | Name   | Type       | Params | Params per Device\n",
      "----------------------------------------------------------\n",
      "0 | emb    | Embedding  | 102 M  | 51.5 M           \n",
      "1 | blocks | ModuleList | 1.3 B  | 654 M            \n",
      "2 | ln_out | LayerNorm  | 4.1 K  | 2.0 K            \n",
      "3 | head   | Linear     | 102 M  | 51.5 M           \n",
      "----------------------------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                         | 0/2654 [00:00<?, ?it/s][2023-07-10 21:52:15,353] [WARNING] [parameter_offload.py:86:_apply_to_tensors_only] A module has unknown inputs or outputs type (<class 'src.model.BlockState'>) and the tensors embedded in it cannot be detected. The ZeRO-3 hooks designed to trigger before or after backward pass of the module relies on knowing the input and output tensors and therefore may not get triggered properly.\n",
      "Epoch 0:   0%| | 9/2654 [00:12<1:03:18,  1.44s/it, v_num=hk35, train/loss=10.90][2023-07-10 21:52:27,545] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   1%|  | 19/2654 [00:24<55:45,  1.27s/it, v_num=hk35, train/loss=9.440][2023-07-10 21:52:38,695] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   1%|  | 29/2654 [00:35<53:16,  1.22s/it, v_num=hk35, train/loss=9.310][2023-07-10 21:52:49,953] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   1%|  | 39/2654 [00:46<52:02,  1.19s/it, v_num=hk35, train/loss=8.880][2023-07-10 21:53:01,142] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   2%|  | 49/2654 [00:57<51:10,  1.18s/it, v_num=hk35, train/loss=8.440][2023-07-10 21:53:12,393] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   2%|  | 59/2654 [01:08<50:34,  1.17s/it, v_num=hk35, train/loss=8.310][2023-07-10 21:53:23,578] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   3%|  | 69/2654 [01:20<50:05,  1.16s/it, v_num=hk35, train/loss=8.440][2023-07-10 21:53:34,875] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   3%|  | 79/2654 [01:31<49:43,  1.16s/it, v_num=hk35, train/loss=7.840][2023-07-10 21:53:46,127] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   3%|  | 89/2654 [01:42<49:22,  1.15s/it, v_num=hk35, train/loss=8.750][2023-07-10 21:53:57,434] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   4%|  | 99/2654 [01:54<49:03,  1.15s/it, v_num=hk35, train/loss=8.310][2023-07-10 21:54:08,634] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   4%| | 109/2654 [02:05<48:44,  1.15s/it, v_num=hk35, train/loss=8.380][2023-07-10 21:54:19,913] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   4%| | 119/2654 [02:16<48:28,  1.15s/it, v_num=hk35, train/loss=8.940][2023-07-10 21:54:31,134] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   5%| | 129/2654 [02:27<48:12,  1.15s/it, v_num=hk35, train/loss=8.380][2023-07-10 21:54:42,445] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   5%| | 139/2654 [02:39<47:58,  1.14s/it, v_num=hk35, train/loss=8.190][2023-07-10 21:54:53,669] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   6%| | 149/2654 [02:50<47:44,  1.14s/it, v_num=hk35, train/loss=8.000][2023-07-10 21:55:05,026] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   6%| | 159/2654 [03:01<47:30,  1.14s/it, v_num=hk35, train/loss=8.060][2023-07-10 21:55:16,241] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   6%| | 169/2654 [03:12<47:16,  1.14s/it, v_num=hk35, train/loss=7.940][2023-07-10 21:55:27,556] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   7%| | 179/2654 [03:24<47:03,  1.14s/it, v_num=hk35, train/loss=7.970][2023-07-10 21:55:38,748] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   7%| | 189/2654 [03:35<46:48,  1.14s/it, v_num=hk35, train/loss=8.810][2023-07-10 21:55:50,034] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   7%| | 199/2654 [03:46<46:36,  1.14s/it, v_num=hk35, train/loss=7.660][2023-07-10 21:56:01,273] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   8%| | 209/2654 [03:57<46:23,  1.14s/it, v_num=hk35, train/loss=7.590][2023-07-10 21:56:12,582] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   8%| | 219/2654 [04:09<46:10,  1.14s/it, v_num=hk35, train/loss=7.910][2023-07-10 21:56:23,779] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   9%| | 229/2654 [04:20<45:57,  1.14s/it, v_num=hk35, train/loss=7.810][2023-07-10 21:56:35,039] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   9%| | 239/2654 [04:31<45:45,  1.14s/it, v_num=hk35, train/loss=7.410][2023-07-10 21:56:46,272] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   9%| | 249/2654 [04:42<45:32,  1.14s/it, v_num=hk35, train/loss=7.910][2023-07-10 21:56:57,570] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  10%| | 259/2654 [04:54<45:20,  1.14s/it, v_num=hk35, train/loss=7.720][2023-07-10 21:57:08,760] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  10%| | 269/2654 [05:05<45:07,  1.14s/it, v_num=hk35, train/loss=7.780][2023-07-10 21:57:20,041] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  11%| | 279/2654 [05:16<44:55,  1.13s/it, v_num=hk35, train/loss=7.810][2023-07-10 21:57:31,226] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  11%| | 289/2654 [05:27<44:42,  1.13s/it, v_num=hk35, train/loss=7.660][2023-07-10 21:57:42,485] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  11%| | 299/2654 [05:39<44:30,  1.13s/it, v_num=hk35, train/loss=7.590][2023-07-10 21:57:53,682] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  12%| | 309/2654 [05:50<44:19,  1.13s/it, v_num=hk35, train/loss=7.620][2023-07-10 21:58:05,024] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  12%| | 319/2654 [06:01<44:07,  1.13s/it, v_num=hk35, train/loss=7.690][2023-07-10 21:58:16,304] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  12%| | 329/2654 [06:12<43:55,  1.13s/it, v_num=hk35, train/loss=7.530][2023-07-10 21:58:27,641] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  13%|‚ñè| 339/2654 [06:24<43:44,  1.13s/it, v_num=hk35, train/loss=8.190][2023-07-10 21:58:38,891] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  13%|‚ñè| 349/2654 [06:35<43:31,  1.13s/it, v_num=hk35, train/loss=7.530][2023-07-10 21:58:50,088] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  14%|‚ñè| 359/2654 [06:46<43:19,  1.13s/it, v_num=hk35, train/loss=7.690][2023-07-10 21:59:01,260] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  14%|‚ñè| 369/2654 [06:57<43:07,  1.13s/it, v_num=hk35, train/loss=8.190][2023-07-10 21:59:12,513] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  14%|‚ñè| 379/2654 [07:09<42:55,  1.13s/it, v_num=hk35, train/loss=7.590][2023-07-10 21:59:23,706] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  15%|‚ñè| 389/2654 [07:20<42:43,  1.13s/it, v_num=hk35, train/loss=7.840][2023-07-10 21:59:34,977] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  15%|‚ñè| 399/2654 [07:31<42:32,  1.13s/it, v_num=hk35, train/loss=7.120][2023-07-10 21:59:46,179] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  15%|‚ñè| 409/2654 [07:42<42:20,  1.13s/it, v_num=hk35, train/loss=7.660][2023-07-10 21:59:57,460] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  16%|‚ñè| 419/2654 [07:54<42:08,  1.13s/it, v_num=hk35, train/loss=7.470][2023-07-10 22:00:08,664] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  16%|‚ñè| 429/2654 [08:05<41:56,  1.13s/it, v_num=hk35, train/loss=7.810][2023-07-10 22:00:19,933] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  17%|‚ñè| 439/2654 [08:16<41:45,  1.13s/it, v_num=hk35, train/loss=7.560][2023-07-10 22:00:31,107] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  17%|‚ñè| 449/2654 [08:27<41:33,  1.13s/it, v_num=hk35, train/loss=7.940][2023-07-10 22:00:42,373] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  17%|‚ñè| 459/2654 [08:39<41:22,  1.13s/it, v_num=hk35, train/loss=7.880][2023-07-10 22:00:53,695] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  18%|‚ñè| 469/2654 [08:50<41:11,  1.13s/it, v_num=hk35, train/loss=7.340][2023-07-10 22:01:05,085] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  18%|‚ñè| 479/2654 [09:01<41:00,  1.13s/it, v_num=hk35, train/loss=7.310][2023-07-10 22:01:16,417] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  18%|‚ñè| 489/2654 [09:13<40:49,  1.13s/it, v_num=hk35, train/loss=7.410][2023-07-10 22:01:27,839] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  19%|‚ñè| 499/2654 [09:24<40:37,  1.13s/it, v_num=hk35, train/loss=7.440][2023-07-10 22:01:39,083] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  19%|‚ñè| 509/2654 [09:35<40:26,  1.13s/it, v_num=hk35, train/loss=7.780][2023-07-10 22:01:50,429] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  20%|‚ñè| 519/2654 [09:47<40:15,  1.13s/it, v_num=hk35, train/loss=7.190][2023-07-10 22:02:01,703] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  20%|‚ñè| 529/2654 [09:58<40:03,  1.13s/it, v_num=hk35, train/loss=7.220][2023-07-10 22:02:13,056] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  20%|‚ñè| 539/2654 [10:09<39:52,  1.13s/it, v_num=hk35, train/loss=7.000][2023-07-10 22:02:24,259] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  21%|‚ñè| 549/2654 [10:20<39:41,  1.13s/it, v_num=hk35, train/loss=7.090][2023-07-10 22:02:35,649] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  21%|‚ñè| 559/2654 [10:32<39:30,  1.13s/it, v_num=hk35, train/loss=7.470][2023-07-10 22:02:47,013] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  21%|‚ñè| 569/2654 [10:43<39:18,  1.13s/it, v_num=hk35, train/loss=7.250][2023-07-10 22:02:58,306] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  22%|‚ñè| 579/2654 [10:54<39:07,  1.13s/it, v_num=hk35, train/loss=7.380][2023-07-10 22:03:09,548] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  22%|‚ñè| 589/2654 [11:06<38:55,  1.13s/it, v_num=hk35, train/loss=7.500][2023-07-10 22:03:20,909] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  23%|‚ñè| 599/2654 [11:17<38:44,  1.13s/it, v_num=hk35, train/loss=7.250][2023-07-10 22:03:32,076] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  23%|‚ñè| 609/2654 [11:28<38:32,  1.13s/it, v_num=hk35, train/loss=6.720][2023-07-10 22:03:43,277] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  23%|‚ñè| 619/2654 [11:39<38:20,  1.13s/it, v_num=hk35, train/loss=7.380][2023-07-10 22:03:54,426] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  24%|‚ñè| 629/2654 [11:51<38:09,  1.13s/it, v_num=hk35, train/loss=7.750][2023-07-10 22:04:05,750] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  24%|‚ñè| 639/2654 [12:02<37:58,  1.13s/it, v_num=hk35, train/loss=7.190][2023-07-10 22:04:17,012] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  24%|‚ñè| 649/2654 [12:13<37:46,  1.13s/it, v_num=hk35, train/loss=7.090][2023-07-10 22:04:28,353] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  25%|‚ñè| 659/2654 [12:25<37:35,  1.13s/it, v_num=hk35, train/loss=7.220][2023-07-10 22:04:39,644] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  25%|‚ñé| 669/2654 [12:36<37:24,  1.13s/it, v_num=hk35, train/loss=7.280][2023-07-10 22:04:50,981] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  26%|‚ñé| 679/2654 [12:47<37:13,  1.13s/it, v_num=hk35, train/loss=7.060][2023-07-10 22:05:02,348] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  26%|‚ñé| 689/2654 [12:59<37:02,  1.13s/it, v_num=hk35, train/loss=6.590][2023-07-10 22:05:13,813] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  26%|‚ñé| 699/2654 [13:10<36:51,  1.13s/it, v_num=hk35, train/loss=7.120][2023-07-10 22:05:25,198] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  27%|‚ñé| 709/2654 [13:21<36:39,  1.13s/it, v_num=hk35, train/loss=6.470][2023-07-10 22:05:36,538] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  27%|‚ñé| 719/2654 [13:33<36:28,  1.13s/it, v_num=hk35, train/loss=6.910][2023-07-10 22:05:47,870] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  27%|‚ñé| 729/2654 [13:44<36:17,  1.13s/it, v_num=hk35, train/loss=7.190][2023-07-10 22:05:59,116] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  28%|‚ñé| 739/2654 [13:55<36:05,  1.13s/it, v_num=hk35, train/loss=7.120][2023-07-10 22:06:10,441] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  28%|‚ñé| 749/2654 [14:07<35:55,  1.13s/it, v_num=hk35, train/loss=6.720][2023-07-10 22:06:21,993] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  29%|‚ñé| 759/2654 [14:19<35:44,  1.13s/it, v_num=hk35, train/loss=7.030][2023-07-10 22:06:33,729] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  29%|‚ñé| 769/2654 [14:30<35:33,  1.13s/it, v_num=hk35, train/loss=7.060][2023-07-10 22:06:44,934] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  29%|‚ñé| 779/2654 [14:41<35:21,  1.13s/it, v_num=hk35, train/loss=6.840][2023-07-10 22:06:56,221] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  30%|‚ñé| 789/2654 [14:52<35:10,  1.13s/it, v_num=hk35, train/loss=6.910][2023-07-10 22:07:07,444] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  30%|‚ñé| 799/2654 [15:04<34:58,  1.13s/it, v_num=hk35, train/loss=6.880][2023-07-10 22:07:18,714] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  30%|‚ñé| 809/2654 [15:15<34:47,  1.13s/it, v_num=hk35, train/loss=7.250][2023-07-10 22:07:29,993] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  31%|‚ñé| 819/2654 [15:26<34:36,  1.13s/it, v_num=hk35, train/loss=6.470][2023-07-10 22:07:41,377] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  31%|‚ñé| 829/2654 [15:38<34:24,  1.13s/it, v_num=hk35, train/loss=6.690][2023-07-10 22:07:52,597] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  32%|‚ñé| 839/2654 [15:49<34:13,  1.13s/it, v_num=hk35, train/loss=6.940][2023-07-10 22:08:03,917] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  32%|‚ñé| 849/2654 [16:00<34:02,  1.13s/it, v_num=hk35, train/loss=7.190][2023-07-10 22:08:15,235] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  32%|‚ñé| 859/2654 [16:12<33:51,  1.13s/it, v_num=hk35, train/loss=6.840][2023-07-10 22:08:26,693] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  33%|‚ñé| 869/2654 [16:23<33:39,  1.13s/it, v_num=hk35, train/loss=6.660][2023-07-10 22:08:37,992] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  33%|‚ñé| 879/2654 [16:34<33:28,  1.13s/it, v_num=hk35, train/loss=6.590][2023-07-10 22:08:49,357] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  33%|‚ñé| 889/2654 [16:46<33:17,  1.13s/it, v_num=hk35, train/loss=7.410][2023-07-10 22:09:00,752] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  34%|‚ñé| 899/2654 [16:57<33:06,  1.13s/it, v_num=hk35, train/loss=7.090][2023-07-10 22:09:12,165] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  34%|‚ñé| 909/2654 [17:08<32:55,  1.13s/it, v_num=hk35, train/loss=6.470][2023-07-10 22:09:23,515] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  35%|‚ñé| 919/2654 [17:20<32:43,  1.13s/it, v_num=hk35, train/loss=6.590][2023-07-10 22:09:34,737] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  35%|‚ñé| 929/2654 [17:31<32:32,  1.13s/it, v_num=hk35, train/loss=6.750][2023-07-10 22:09:45,948] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  35%|‚ñé| 939/2654 [17:42<32:20,  1.13s/it, v_num=hk35, train/loss=7.000][2023-07-10 22:09:57,229] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  36%|‚ñé| 949/2654 [17:53<32:09,  1.13s/it, v_num=hk35, train/loss=7.000][2023-07-10 22:10:08,527] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  36%|‚ñé| 959/2654 [18:05<31:58,  1.13s/it, v_num=hk35, train/loss=6.410][2023-07-10 22:10:19,993] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  37%|‚ñé| 969/2654 [18:16<31:47,  1.13s/it, v_num=hk35, train/loss=6.560][2023-07-10 22:10:31,269] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  37%|‚ñé| 979/2654 [18:27<31:35,  1.13s/it, v_num=hk35, train/loss=6.910][2023-07-10 22:10:42,542] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  37%|‚ñé| 989/2654 [18:39<31:23,  1.13s/it, v_num=hk35, train/loss=6.840][2023-07-10 22:10:53,646] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  38%|‚ñç| 999/2654 [18:50<31:12,  1.13s/it, v_num=hk35, train/loss=6.690][2023-07-10 22:11:04,825] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  38%|‚ñç| 1009/2654 [19:01<31:00,  1.13s/it, v_num=hk35, train/loss=6.780[2023-07-10 22:11:16,009] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  38%|‚ñç| 1019/2654 [19:12<30:49,  1.13s/it, v_num=hk35, train/loss=6.720[2023-07-10 22:11:27,209] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  39%|‚ñç| 1029/2654 [19:23<30:37,  1.13s/it, v_num=hk35, train/loss=6.340[2023-07-10 22:11:38,419] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  39%|‚ñç| 1039/2654 [19:35<30:26,  1.13s/it, v_num=hk35, train/loss=6.750[2023-07-10 22:11:49,681] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  40%|‚ñç| 1049/2654 [19:46<30:15,  1.13s/it, v_num=hk35, train/loss=6.120[2023-07-10 22:12:00,918] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  40%|‚ñç| 1059/2654 [19:57<30:03,  1.13s/it, v_num=hk35, train/loss=6.750[2023-07-10 22:12:12,215] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  40%|‚ñç| 1069/2654 [20:08<29:52,  1.13s/it, v_num=hk35, train/loss=6.220[2023-07-10 22:12:23,557] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  41%|‚ñç| 1079/2654 [20:20<29:41,  1.13s/it, v_num=hk35, train/loss=6.440[2023-07-10 22:12:35,049] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  41%|‚ñç| 1089/2654 [20:31<29:30,  1.13s/it, v_num=hk35, train/loss=6.440[2023-07-10 22:12:46,293] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  41%|‚ñç| 1099/2654 [20:42<29:18,  1.13s/it, v_num=hk35, train/loss=7.000[2023-07-10 22:12:57,577] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  42%|‚ñç| 1109/2654 [20:54<29:07,  1.13s/it, v_num=hk35, train/loss=6.410[2023-07-10 22:13:08,832] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  42%|‚ñç| 1119/2654 [21:05<28:56,  1.13s/it, v_num=hk35, train/loss=6.590[2023-07-10 22:13:20,273] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  43%|‚ñç| 1129/2654 [21:16<28:44,  1.13s/it, v_num=hk35, train/loss=6.060[2023-07-10 22:13:31,561] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  43%|‚ñç| 1139/2654 [21:28<28:33,  1.13s/it, v_num=hk35, train/loss=6.440[2023-07-10 22:13:42,917] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  43%|‚ñç| 1149/2654 [21:39<28:22,  1.13s/it, v_num=hk35, train/loss=6.500[2023-07-10 22:13:54,198] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  44%|‚ñç| 1159/2654 [21:50<28:10,  1.13s/it, v_num=hk35, train/loss=6.690[2023-07-10 22:14:05,525] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  44%|‚ñç| 1169/2654 [22:02<27:59,  1.13s/it, v_num=hk35, train/loss=6.470[2023-07-10 22:14:16,789] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  44%|‚ñç| 1179/2654 [22:13<27:48,  1.13s/it, v_num=hk35, train/loss=6.060[2023-07-10 22:14:28,123] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  45%|‚ñç| 1189/2654 [22:24<27:36,  1.13s/it, v_num=hk35, train/loss=6.660[2023-07-10 22:14:39,372] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  45%|‚ñç| 1199/2654 [22:35<27:25,  1.13s/it, v_num=hk35, train/loss=6.470[2023-07-10 22:14:50,613] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  46%|‚ñç| 1209/2654 [22:47<27:14,  1.13s/it, v_num=hk35, train/loss=6.000[2023-07-10 22:15:01,851] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  46%|‚ñç| 1219/2654 [22:58<27:02,  1.13s/it, v_num=hk35, train/loss=6.410[2023-07-10 22:15:13,153] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  46%|‚ñç| 1229/2654 [23:09<26:51,  1.13s/it, v_num=hk35, train/loss=6.220[2023-07-10 22:15:24,412] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  47%|‚ñç| 1239/2654 [23:21<26:40,  1.13s/it, v_num=hk35, train/loss=6.250[2023-07-10 22:15:35,700] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  47%|‚ñç| 1249/2654 [23:32<26:28,  1.13s/it, v_num=hk35, train/loss=6.560[2023-07-10 22:15:46,943] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  47%|‚ñç| 1259/2654 [23:43<26:17,  1.13s/it, v_num=hk35, train/loss=6.660[2023-07-10 22:15:58,318] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  48%|‚ñç| 1269/2654 [23:55<26:06,  1.13s/it, v_num=hk35, train/loss=6.660[2023-07-10 22:16:09,709] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  48%|‚ñç| 1279/2654 [24:06<25:55,  1.13s/it, v_num=hk35, train/loss=6.810[2023-07-10 22:16:21,178] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  49%|‚ñç| 1289/2654 [24:18<25:44,  1.13s/it, v_num=hk35, train/loss=6.660[2023-07-10 22:16:32,647] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  49%|‚ñç| 1299/2654 [24:29<25:32,  1.13s/it, v_num=hk35, train/loss=6.250[2023-07-10 22:16:44,045] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  49%|‚ñç| 1309/2654 [24:40<25:21,  1.13s/it, v_num=hk35, train/loss=6.060[2023-07-10 22:16:55,345] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  50%|‚ñç| 1319/2654 [24:52<25:10,  1.13s/it, v_num=hk35, train/loss=6.440[2023-07-10 22:17:06,786] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  50%|‚ñå| 1329/2654 [25:03<24:58,  1.13s/it, v_num=hk35, train/loss=6.560[2023-07-10 22:17:18,016] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  50%|‚ñå| 1339/2654 [25:14<24:47,  1.13s/it, v_num=hk35, train/loss=6.120[2023-07-10 22:17:29,237] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  51%|‚ñå| 1349/2654 [25:25<24:36,  1.13s/it, v_num=hk35, train/loss=6.280[2023-07-10 22:17:40,591] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  51%|‚ñå| 1359/2654 [25:37<24:24,  1.13s/it, v_num=hk35, train/loss=6.340[2023-07-10 22:17:51,865] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  52%|‚ñå| 1369/2654 [25:48<24:13,  1.13s/it, v_num=hk35, train/loss=6.190[2023-07-10 22:18:03,173] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  52%|‚ñå| 1379/2654 [25:59<24:02,  1.13s/it, v_num=hk35, train/loss=7.060[2023-07-10 22:18:14,525] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  52%|‚ñå| 1389/2654 [26:11<23:50,  1.13s/it, v_num=hk35, train/loss=6.590[2023-07-10 22:18:25,750] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  53%|‚ñå| 1399/2654 [26:22<23:39,  1.13s/it, v_num=hk35, train/loss=6.380[2023-07-10 22:18:37,101] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  53%|‚ñå| 1409/2654 [26:33<23:28,  1.13s/it, v_num=hk35, train/loss=6.500[2023-07-10 22:18:48,351] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  53%|‚ñå| 1419/2654 [26:45<23:16,  1.13s/it, v_num=hk35, train/loss=6.220[2023-07-10 22:18:59,654] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  54%|‚ñå| 1429/2654 [26:56<23:05,  1.13s/it, v_num=hk35, train/loss=5.840[2023-07-10 22:19:10,902] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  54%|‚ñå| 1439/2654 [27:07<22:54,  1.13s/it, v_num=hk35, train/loss=6.120[2023-07-10 22:19:22,229] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  55%|‚ñå| 1449/2654 [27:18<22:42,  1.13s/it, v_num=hk35, train/loss=6.660[2023-07-10 22:19:33,414] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  55%|‚ñå| 1459/2654 [27:30<22:31,  1.13s/it, v_num=hk35, train/loss=5.880[2023-07-10 22:19:44,709] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  55%|‚ñå| 1469/2654 [27:41<22:20,  1.13s/it, v_num=hk35, train/loss=5.910[2023-07-10 22:19:55,883] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  56%|‚ñå| 1479/2654 [27:52<22:08,  1.13s/it, v_num=hk35, train/loss=6.810[2023-07-10 22:20:07,117] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  56%|‚ñå| 1489/2654 [28:03<21:57,  1.13s/it, v_num=hk35, train/loss=6.280[2023-07-10 22:20:18,430] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  56%|‚ñå| 1499/2654 [28:15<21:46,  1.13s/it, v_num=hk35, train/loss=5.810[2023-07-10 22:20:29,753] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  57%|‚ñå| 1509/2654 [28:26<21:34,  1.13s/it, v_num=hk35, train/loss=6.590[2023-07-10 22:20:41,025] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  57%|‚ñå| 1519/2654 [28:37<21:23,  1.13s/it, v_num=hk35, train/loss=6.000[2023-07-10 22:20:52,363] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  58%|‚ñå| 1529/2654 [28:49<21:12,  1.13s/it, v_num=hk35, train/loss=6.060[2023-07-10 22:21:03,604] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  58%|‚ñå| 1539/2654 [29:00<21:00,  1.13s/it, v_num=hk35, train/loss=6.380[2023-07-10 22:21:14,925] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  58%|‚ñå| 1549/2654 [29:11<20:49,  1.13s/it, v_num=hk35, train/loss=6.220[2023-07-10 22:21:26,313] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  59%|‚ñå| 1559/2654 [29:22<20:38,  1.13s/it, v_num=hk35, train/loss=6.500[2023-07-10 22:21:37,569] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  59%|‚ñå| 1569/2654 [29:34<20:26,  1.13s/it, v_num=hk35, train/loss=6.410[2023-07-10 22:21:48,820] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  59%|‚ñå| 1579/2654 [29:45<20:15,  1.13s/it, v_num=hk35, train/loss=6.160[2023-07-10 22:22:00,152] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  60%|‚ñå| 1589/2654 [29:56<20:04,  1.13s/it, v_num=hk35, train/loss=6.440[2023-07-10 22:22:11,384] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  60%|‚ñå| 1599/2654 [30:08<19:52,  1.13s/it, v_num=hk35, train/loss=6.310[2023-07-10 22:22:22,711] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  61%|‚ñå| 1609/2654 [30:19<19:41,  1.13s/it, v_num=hk35, train/loss=7.440[2023-07-10 22:22:33,943] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  61%|‚ñå| 1619/2654 [30:30<19:30,  1.13s/it, v_num=hk35, train/loss=5.840[2023-07-10 22:22:45,278] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  61%|‚ñå| 1629/2654 [30:42<19:19,  1.13s/it, v_num=hk35, train/loss=6.030[2023-07-10 22:22:56,639] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  62%|‚ñå| 1639/2654 [30:53<19:07,  1.13s/it, v_num=hk35, train/loss=6.280[2023-07-10 22:23:08,040] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  62%|‚ñå| 1649/2654 [31:04<18:56,  1.13s/it, v_num=hk35, train/loss=6.090[2023-07-10 22:23:19,427] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  63%|‚ñã| 1659/2654 [31:16<18:45,  1.13s/it, v_num=hk35, train/loss=6.030[2023-07-10 22:23:30,883] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  63%|‚ñã| 1669/2654 [31:27<18:33,  1.13s/it, v_num=hk35, train/loss=5.810[2023-07-10 22:23:42,142] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  63%|‚ñã| 1679/2654 [31:38<18:22,  1.13s/it, v_num=hk35, train/loss=5.840[2023-07-10 22:23:53,573] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  64%|‚ñã| 1689/2654 [31:50<18:11,  1.13s/it, v_num=hk35, train/loss=6.280[2023-07-10 22:24:04,876] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  64%|‚ñã| 1699/2654 [32:01<18:00,  1.13s/it, v_num=hk35, train/loss=6.250[2023-07-10 22:24:16,269] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  64%|‚ñã| 1709/2654 [32:12<17:48,  1.13s/it, v_num=hk35, train/loss=7.380[2023-07-10 22:24:27,538] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  65%|‚ñã| 1719/2654 [32:24<17:37,  1.13s/it, v_num=hk35, train/loss=5.620[2023-07-10 22:24:38,866] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  65%|‚ñã| 1729/2654 [32:35<17:26,  1.13s/it, v_num=hk35, train/loss=6.280[2023-07-10 22:24:50,204] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  66%|‚ñã| 1739/2654 [32:46<17:14,  1.13s/it, v_num=hk35, train/loss=6.120[2023-07-10 22:25:01,578] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  66%|‚ñã| 1749/2654 [32:58<17:03,  1.13s/it, v_num=hk35, train/loss=5.910[2023-07-10 22:25:12,928] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  66%|‚ñã| 1759/2654 [33:09<16:52,  1.13s/it, v_num=hk35, train/loss=6.250[2023-07-10 22:25:24,343] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  67%|‚ñã| 1769/2654 [33:21<16:41,  1.13s/it, v_num=hk35, train/loss=5.720[2023-07-10 22:25:35,588] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  67%|‚ñã| 1779/2654 [33:32<16:29,  1.13s/it, v_num=hk35, train/loss=6.090[2023-07-10 22:25:46,927] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  67%|‚ñã| 1789/2654 [33:43<16:18,  1.13s/it, v_num=hk35, train/loss=6.220[2023-07-10 22:25:58,230] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  68%|‚ñã| 1799/2654 [33:54<16:07,  1.13s/it, v_num=hk35, train/loss=6.660[2023-07-10 22:26:09,581] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  68%|‚ñã| 1809/2654 [34:06<15:55,  1.13s/it, v_num=hk35, train/loss=6.220[2023-07-10 22:26:21,121] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  69%|‚ñã| 1819/2654 [34:18<15:44,  1.13s/it, v_num=hk35, train/loss=5.410[2023-07-10 22:26:32,953] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  69%|‚ñã| 1829/2654 [34:29<15:33,  1.13s/it, v_num=hk35, train/loss=5.970[2023-07-10 22:26:44,268] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  69%|‚ñã| 1839/2654 [34:40<15:22,  1.13s/it, v_num=hk35, train/loss=6.410[2023-07-10 22:26:55,633] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  70%|‚ñã| 1849/2654 [34:52<15:10,  1.13s/it, v_num=hk35, train/loss=5.970[2023-07-10 22:27:06,846] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  70%|‚ñã| 1859/2654 [35:03<14:59,  1.13s/it, v_num=hk35, train/loss=5.840[2023-07-10 22:27:18,153] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  70%|‚ñã| 1869/2654 [35:14<14:48,  1.13s/it, v_num=hk35, train/loss=5.380[2023-07-10 22:27:29,397] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  71%|‚ñã| 1879/2654 [35:26<14:36,  1.13s/it, v_num=hk35, train/loss=6.030[2023-07-10 22:27:40,695] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  71%|‚ñã| 1889/2654 [35:37<14:25,  1.13s/it, v_num=hk35, train/loss=6.160[2023-07-10 22:27:51,898] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  72%|‚ñã| 1899/2654 [35:48<14:14,  1.13s/it, v_num=hk35, train/loss=6.190[2023-07-10 22:28:03,188] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  72%|‚ñã| 1909/2654 [35:59<14:02,  1.13s/it, v_num=hk35, train/loss=5.250[2023-07-10 22:28:14,419] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  72%|‚ñã| 1919/2654 [36:11<13:51,  1.13s/it, v_num=hk35, train/loss=6.030[2023-07-10 22:28:25,885] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  73%|‚ñã| 1929/2654 [36:22<13:40,  1.13s/it, v_num=hk35, train/loss=6.190[2023-07-10 22:28:37,288] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  73%|‚ñã| 1939/2654 [36:34<13:29,  1.13s/it, v_num=hk35, train/loss=6.440[2023-07-10 22:28:48,745] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  73%|‚ñã| 1949/2654 [36:45<13:17,  1.13s/it, v_num=hk35, train/loss=6.090[2023-07-10 22:29:00,118] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  74%|‚ñã| 1959/2654 [36:56<13:06,  1.13s/it, v_num=hk35, train/loss=6.160[2023-07-10 22:29:11,577] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  74%|‚ñã| 1969/2654 [37:08<12:55,  1.13s/it, v_num=hk35, train/loss=8.750[2023-07-10 22:29:22,924] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  75%|‚ñã| 1979/2654 [37:19<12:43,  1.13s/it, v_num=hk35, train/loss=6.190[2023-07-10 22:29:34,275] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  75%|‚ñã| 1989/2654 [37:30<12:32,  1.13s/it, v_num=hk35, train/loss=6.090[2023-07-10 22:29:45,560] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  75%|‚ñä| 1999/2654 [37:42<12:21,  1.13s/it, v_num=hk35, train/loss=5.620[2023-07-10 22:29:56,929] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  76%|‚ñä| 2009/2654 [37:53<12:09,  1.13s/it, v_num=hk35, train/loss=6.280[2023-07-10 22:30:08,217] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  76%|‚ñä| 2019/2654 [38:04<11:58,  1.13s/it, v_num=hk35, train/loss=6.160[2023-07-10 22:30:19,485] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  76%|‚ñä| 2029/2654 [38:16<11:47,  1.13s/it, v_num=hk35, train/loss=5.720[2023-07-10 22:30:30,813] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  77%|‚ñä| 2039/2654 [38:27<11:35,  1.13s/it, v_num=hk35, train/loss=5.690[2023-07-10 22:30:42,129] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  77%|‚ñä| 2049/2654 [38:38<11:24,  1.13s/it, v_num=hk35, train/loss=6.160[2023-07-10 22:30:53,384] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  78%|‚ñä| 2059/2654 [38:50<11:13,  1.13s/it, v_num=hk35, train/loss=5.690[2023-07-10 22:31:04,745] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  78%|‚ñä| 2069/2654 [39:01<11:02,  1.13s/it, v_num=hk35, train/loss=5.840[2023-07-10 22:31:16,025] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  78%|‚ñä| 2079/2654 [39:12<10:50,  1.13s/it, v_num=hk35, train/loss=5.560[2023-07-10 22:31:27,390] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  79%|‚ñä| 2089/2654 [39:24<10:39,  1.13s/it, v_num=hk35, train/loss=5.720[2023-07-10 22:31:38,638] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  79%|‚ñä| 2099/2654 [39:35<10:28,  1.13s/it, v_num=hk35, train/loss=6.250[2023-07-10 22:31:49,889] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  79%|‚ñä| 2109/2654 [39:46<10:16,  1.13s/it, v_num=hk35, train/loss=5.910[2023-07-10 22:32:01,038] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  80%|‚ñä| 2119/2654 [39:57<10:05,  1.13s/it, v_num=hk35, train/loss=6.160[2023-07-10 22:32:12,201] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  80%|‚ñä| 2129/2654 [40:08<09:53,  1.13s/it, v_num=hk35, train/loss=4.340[2023-07-10 22:32:23,308] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  81%|‚ñä| 2139/2654 [40:20<09:42,  1.13s/it, v_num=hk35, train/loss=5.880[2023-07-10 22:32:34,711] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  81%|‚ñä| 2149/2654 [40:31<09:31,  1.13s/it, v_num=hk35, train/loss=5.410[2023-07-10 22:32:45,989] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  81%|‚ñä| 2159/2654 [40:42<09:20,  1.13s/it, v_num=hk35, train/loss=6.060[2023-07-10 22:32:57,285] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  82%|‚ñä| 2169/2654 [40:54<09:08,  1.13s/it, v_num=hk35, train/loss=5.970[2023-07-10 22:33:08,645] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  82%|‚ñä| 2179/2654 [41:05<08:57,  1.13s/it, v_num=hk35, train/loss=5.810[2023-07-10 22:33:20,033] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  82%|‚ñä| 2189/2654 [41:16<08:46,  1.13s/it, v_num=hk35, train/loss=6.120[2023-07-10 22:33:31,312] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  83%|‚ñä| 2199/2654 [41:27<08:34,  1.13s/it, v_num=hk35, train/loss=5.720[2023-07-10 22:33:42,553] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  83%|‚ñä| 2209/2654 [41:39<08:23,  1.13s/it, v_num=hk35, train/loss=5.910[2023-07-10 22:33:53,755] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  84%|‚ñä| 2219/2654 [41:50<08:12,  1.13s/it, v_num=hk35, train/loss=5.380[2023-07-10 22:34:05,014] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  84%|‚ñä| 2229/2654 [42:01<08:00,  1.13s/it, v_num=hk35, train/loss=5.500[2023-07-10 22:34:16,286] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  84%|‚ñä| 2239/2654 [42:12<07:49,  1.13s/it, v_num=hk35, train/loss=5.440[2023-07-10 22:34:27,645] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  85%|‚ñä| 2249/2654 [42:24<07:38,  1.13s/it, v_num=hk35, train/loss=5.720[2023-07-10 22:34:39,136] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  85%|‚ñä| 2259/2654 [42:35<07:26,  1.13s/it, v_num=hk35, train/loss=5.310[2023-07-10 22:34:50,478] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  85%|‚ñä| 2269/2654 [42:47<07:15,  1.13s/it, v_num=hk35, train/loss=5.250[2023-07-10 22:35:01,680] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  86%|‚ñä| 2279/2654 [42:58<07:04,  1.13s/it, v_num=hk35, train/loss=2.890[2023-07-10 22:35:12,925] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  86%|‚ñä| 2289/2654 [43:09<06:52,  1.13s/it, v_num=hk35, train/loss=5.250[2023-07-10 22:35:24,304] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  87%|‚ñä| 2299/2654 [43:21<06:41,  1.13s/it, v_num=hk35, train/loss=5.380[2023-07-10 22:35:35,757] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  87%|‚ñä| 2309/2654 [43:32<06:30,  1.13s/it, v_num=hk35, train/loss=5.940[2023-07-10 22:35:47,053] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  87%|‚ñä| 2319/2654 [43:43<06:19,  1.13s/it, v_num=hk35, train/loss=5.280[2023-07-10 22:35:58,413] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  88%|‚ñâ| 2329/2654 [43:55<06:07,  1.13s/it, v_num=hk35, train/loss=5.500[2023-07-10 22:36:09,706] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  88%|‚ñâ| 2339/2654 [44:06<05:56,  1.13s/it, v_num=hk35, train/loss=6.000[2023-07-10 22:36:21,229] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  89%|‚ñâ| 2349/2654 [44:17<05:45,  1.13s/it, v_num=hk35, train/loss=5.560[2023-07-10 22:36:32,477] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  89%|‚ñâ| 2359/2654 [44:29<05:33,  1.13s/it, v_num=hk35, train/loss=5.880[2023-07-10 22:36:43,866] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  89%|‚ñâ| 2369/2654 [44:40<05:22,  1.13s/it, v_num=hk35, train/loss=6.120[2023-07-10 22:36:55,220] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  90%|‚ñâ| 2379/2654 [44:52<05:11,  1.13s/it, v_num=hk35, train/loss=5.120[2023-07-10 22:37:06,706] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  90%|‚ñâ| 2389/2654 [45:03<04:59,  1.13s/it, v_num=hk35, train/loss=4.750[2023-07-10 22:37:18,021] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  90%|‚ñâ| 2399/2654 [45:14<04:48,  1.13s/it, v_num=hk35, train/loss=6.030[2023-07-10 22:37:29,421] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  91%|‚ñâ| 2409/2654 [45:26<04:37,  1.13s/it, v_num=hk35, train/loss=5.810[2023-07-10 22:37:40,743] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  91%|‚ñâ| 2419/2654 [45:37<04:25,  1.13s/it, v_num=hk35, train/loss=5.810[2023-07-10 22:37:52,197] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  92%|‚ñâ| 2429/2654 [45:48<04:14,  1.13s/it, v_num=hk35, train/loss=5.530[2023-07-10 22:38:03,481] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  92%|‚ñâ| 2439/2654 [46:00<04:03,  1.13s/it, v_num=hk35, train/loss=5.840[2023-07-10 22:38:14,861] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  92%|‚ñâ| 2449/2654 [46:11<03:52,  1.13s/it, v_num=hk35, train/loss=5.940[2023-07-10 22:38:26,179] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  93%|‚ñâ| 2459/2654 [46:22<03:40,  1.13s/it, v_num=hk35, train/loss=5.690[2023-07-10 22:38:37,681] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  93%|‚ñâ| 2469/2654 [46:34<03:29,  1.13s/it, v_num=hk35, train/loss=5.470[2023-07-10 22:38:49,131] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  93%|‚ñâ| 2479/2654 [46:45<03:18,  1.13s/it, v_num=hk35, train/loss=3.700[2023-07-10 22:39:00,473] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  94%|‚ñâ| 2489/2654 [46:57<03:06,  1.13s/it, v_num=hk35, train/loss=5.690[2023-07-10 22:39:11,799] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  94%|‚ñâ| 2499/2654 [47:08<02:55,  1.13s/it, v_num=hk35, train/loss=6.470[2023-07-10 22:39:23,145] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  95%|‚ñâ| 2509/2654 [47:19<02:44,  1.13s/it, v_num=hk35, train/loss=5.440[2023-07-10 22:39:34,446] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  95%|‚ñâ| 2519/2654 [47:31<02:32,  1.13s/it, v_num=hk35, train/loss=5.590[2023-07-10 22:39:45,813] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  95%|‚ñâ| 2529/2654 [47:42<02:21,  1.13s/it, v_num=hk35, train/loss=5.940[2023-07-10 22:39:57,083] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  96%|‚ñâ| 2539/2654 [47:53<02:10,  1.13s/it, v_num=hk35, train/loss=5.380[2023-07-10 22:40:08,459] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  96%|‚ñâ| 2549/2654 [48:05<01:58,  1.13s/it, v_num=hk35, train/loss=6.000[2023-07-10 22:40:19,836] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  96%|‚ñâ| 2559/2654 [48:16<01:47,  1.13s/it, v_num=hk35, train/loss=5.340[2023-07-10 22:40:31,201] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  97%|‚ñâ| 2569/2654 [48:27<01:36,  1.13s/it, v_num=hk35, train/loss=5.280[2023-07-10 22:40:42,540] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  97%|‚ñâ| 2579/2654 [48:39<01:24,  1.13s/it, v_num=hk35, train/loss=5.970[2023-07-10 22:40:54,033] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  98%|‚ñâ| 2589/2654 [48:50<01:13,  1.13s/it, v_num=hk35, train/loss=5.470[2023-07-10 22:41:05,393] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  98%|‚ñâ| 2599/2654 [49:02<01:02,  1.13s/it, v_num=hk35, train/loss=5.250[2023-07-10 22:41:16,745] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  98%|‚ñâ| 2609/2654 [49:13<00:50,  1.13s/it, v_num=hk35, train/loss=4.160[2023-07-10 22:41:28,049] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  99%|‚ñâ| 2619/2654 [49:24<00:39,  1.13s/it, v_num=hk35, train/loss=6.220[2023-07-10 22:41:39,405] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  99%|‚ñâ| 2629/2654 [49:36<00:28,  1.13s/it, v_num=hk35, train/loss=5.780[2023-07-10 22:41:50,716] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  99%|‚ñâ| 2639/2654 [49:47<00:16,  1.13s/it, v_num=hk35, train/loss=5.720[2023-07-10 22:42:02,073] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0: 100%|‚ñâ| 2649/2654 [49:58<00:05,  1.13s/it, v_num=hk35, train/loss=6.090[2023-07-10 22:42:13,337] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [50:04<00:00,  1.13s/it, v_num=hk35, train/loss=6.060\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|‚ñã                  | 1/27 [00:00<00:13,  1.90it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|‚ñà‚ñç                 | 2/27 [00:01<00:13,  1.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|‚ñà‚ñà                 | 3/27 [00:01<00:12,  1.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|‚ñà‚ñà‚ñä                | 4/27 [00:02<00:12,  1.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|‚ñà‚ñà‚ñà‚ñå               | 5/27 [00:02<00:11,  1.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|‚ñà‚ñà‚ñà‚ñà‚ñè              | 6/27 [00:03<00:11,  1.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|‚ñà‚ñà‚ñà‚ñà‚ñâ              | 7/27 [00:03<00:10,  1.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 8/27 [00:04<00:10,  1.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 9/27 [00:04<00:09,  1.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 10/27 [00:05<00:08,  1.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 11/27 [00:05<00:08,  1.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 12/27 [00:06<00:07,  1.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 13/27 [00:06<00:07,  1.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 14/27 [00:07<00:06,  1.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 15/27 [00:07<00:06,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 16/27 [00:08<00:05,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 17/27 [00:08<00:05,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 18/27 [00:09<00:04,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 19/27 [00:09<00:04,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 20/27 [00:10<00:03,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 21/27 [00:10<00:03,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 22/27 [00:11<00:02,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 23/27 [00:11<00:02,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/27 [00:12<00:01,  1.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 25/27 [00:12<00:01,  1.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/27 [00:13<00:00,  1.93it/s]\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [50:20<00:00,  1.14s/it, v_num=hk35, train/loss=6.060\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [50:20<00:00,  1.14s/it, v_num=hk35, train/loss=6.060`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [50:20<00:00,  1.14s/it, v_num=hk35, train/loss=6.060\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 5.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 5.6088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33minfctx-deepspeed-deterministic (deepspeed_stage_3, train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/7jwlhk35\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230710_215131-7jwlhk35/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ../../RWKV-v4neo && \\\n",
    "    export RWKV_JIT_ON=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c ../notebook/trainer-validation/config/baseline-1024.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_3, train-ctx=1024, data-ctx=1024)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_3\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 3 + offload\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 3 + offload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230710_224256-9hkg3yg1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-deepspeed-deterministic (deepspeed_stage_3_offload, train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/9hkg3yg1\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 778.45it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-d9ebc51d67d5ce31_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c63aa2ba2a5c9680_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-193f68ea4e8054de_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-07-10 22:43:11,961] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-07-10 22:43:27,412] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-10 22:43:27,451] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: it is highly recommended to enable bptt_learning when used to deepspeed 2/3/offloading, otherwise an exception will occur when training with dataset records, larger then the configured context length (1024)\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: it is highly recommended to enable bptt_learning when used to deepspeed 2/3/offloading, otherwise an exception will occur when training with dataset records, larger then the configured context length (1024)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.378918170928955 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.4016318321228027 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.08635473251342773 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07927584648132324 seconds\n",
      "Parameter Offload: Total persistent parameters: 548864 in 268 params\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00030994415283203125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006022453308105469 seconds\n",
      "\n",
      "  | Name   | Type       | Params | Params per Device\n",
      "----------------------------------------------------------\n",
      "0 | emb    | Embedding  | 102 M  | 51.5 M           \n",
      "1 | blocks | ModuleList | 1.3 B  | 654 M            \n",
      "2 | ln_out | LayerNorm  | 4.1 K  | 2.0 K            \n",
      "3 | head   | Linear     | 102 M  | 51.5 M           \n",
      "----------------------------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                         | 0/2654 [00:00<?, ?it/s][2023-07-10 22:43:55,299] [WARNING] [parameter_offload.py:86:_apply_to_tensors_only] A module has unknown inputs or outputs type (<class 'src.model.BlockState'>) and the tensors embedded in it cannot be detected. The ZeRO-3 hooks designed to trigger before or after backward pass of the module relies on knowing the input and output tensors and therefore may not get triggered properly.\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [1:28:53<00:00,  2.01s/it, v_num=3yg1, train/loss=6.0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|‚ñã                  | 1/27 [00:00<00:17,  1.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|‚ñà‚ñç                 | 2/27 [00:01<00:18,  1.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|‚ñà‚ñà                 | 3/27 [00:02<00:16,  1.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|‚ñà‚ñà‚ñä                | 4/27 [00:02<00:15,  1.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|‚ñà‚ñà‚ñà‚ñå               | 5/27 [00:03<00:14,  1.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|‚ñà‚ñà‚ñà‚ñà‚ñè              | 6/27 [00:03<00:13,  1.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|‚ñà‚ñà‚ñà‚ñà‚ñâ              | 7/27 [00:04<00:12,  1.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 8/27 [00:05<00:12,  1.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 9/27 [00:05<00:11,  1.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 10/27 [00:06<00:10,  1.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 11/27 [00:06<00:10,  1.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 12/27 [00:07<00:09,  1.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 13/27 [00:08<00:08,  1.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 14/27 [00:08<00:08,  1.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 15/27 [00:09<00:07,  1.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 16/27 [00:09<00:06,  1.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 17/27 [00:10<00:06,  1.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 18/27 [00:11<00:05,  1.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 19/27 [00:11<00:04,  1.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 20/27 [00:12<00:04,  1.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 21/27 [00:12<00:03,  1.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 22/27 [00:13<00:03,  1.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 23/27 [00:14<00:02,  1.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/27 [00:14<00:01,  1.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 25/27 [00:15<00:01,  1.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/27 [00:15<00:00,  1.63it/s]\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [1:29:15<00:00,  2.02s/it, v_num=3yg1, train/loss=6.0\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [1:29:15<00:00,  2.02s/it, v_num=3yg1, train/loss=6.0`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|‚ñà| 2654/2654 [1:29:15<00:00,  2.02s/it, v_num=3yg1, train/loss=6.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 5.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 5.6059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33minfctx-deepspeed-deterministic (deepspeed_stage_3_offload, train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/9hkg3yg1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230710_224256-9hkg3yg1/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ../../RWKV-v4neo && \\\n",
    "    export RWKV_JIT_ON=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c ../notebook/trainer-validation/config/baseline-1024.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_3_offload, train-ctx=1024, data-ctx=1024)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_3_offload\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
