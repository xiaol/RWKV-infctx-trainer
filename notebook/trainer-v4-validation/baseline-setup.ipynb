{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfCtx trainer baseline setup\n",
    "The trainer validation is mostly done using the either of the following\n",
    "\n",
    "**1B5 model wtih**\n",
    "- Layer count: 24\n",
    "- Embed size: 2048\n",
    "\n",
    "**L6-D512 model with**\n",
    "- Layer count: 6\n",
    "- Embed size: 512\n",
    "\n",
    "Typically with the following dataset\n",
    "- \"teven/enwiki_10k\" dataset, chunked to 1024 token sizes\n",
    "\n",
    "The following notebook, helps perform the basic download and setup for the \"init model\", and \"test dataset\". Which is used as a reference point for all other validation processes (unless stated otherwise)\n",
    "\n",
    "Generally you only need to do this once\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps\n",
    ">\n",
    "> All training runs (except dryrun) is configured to log to weights and bias, comment out the logger in the config file if you want to avoid this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "## Preparing the init model and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘Echo-A-1B5-Init.pth’ already there; not retrieving.\n",
      "\n",
      "-rw-rw-r-- 1 picocreator picocreator 2.9G Jun 22 12:41 ../../model/Echo-A-1B5-Init.pth\n"
     ]
    }
   ],
   "source": [
    "# First lets setup the various directories, and get the blank init model, these init model was generated\n",
    "# using the original RWKV-LM repo (as at this point of writing, this repo cannot init a model)\n",
    "# As such I have preinitialized these blank models and uploaded them to HF for convinence\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/\n",
    "!cd ../../model/ && wget -nc https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
    "!ls -alh ../../model/Echo-A-1B5-Init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-30 14:30:20,457] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_2048_bf16/build.ninja...\n",
      "Building extension module wkv_2048_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_2048_bf16...\n",
      "50277 512   -0.1 emb.weight\n",
      "512   512   0    blocks.0.att.key.weight\n",
      "512   512   1.0  blocks.0.att.value.weight\n",
      "512   512   0    blocks.0.att.receptance.weight\n",
      "512   512   0    blocks.0.att.output.weight\n",
      "2048  512   1.0  blocks.0.ffn.key.weight\n",
      "512   512   0    blocks.0.ffn.receptance.weight\n",
      "512   2048  0    blocks.0.ffn.value.weight\n",
      "512   512   0    blocks.1.att.key.weight\n",
      "512   512   1.0  blocks.1.att.value.weight\n",
      "512   512   0    blocks.1.att.receptance.weight\n",
      "512   512   0    blocks.1.att.output.weight\n",
      "2048  512   1.0  blocks.1.ffn.key.weight\n",
      "512   512   0    blocks.1.ffn.receptance.weight\n",
      "512   2048  0    blocks.1.ffn.value.weight\n",
      "512   512   0    blocks.2.att.key.weight\n",
      "512   512   1.0  blocks.2.att.value.weight\n",
      "512   512   0    blocks.2.att.receptance.weight\n",
      "512   512   0    blocks.2.att.output.weight\n",
      "2048  512   1.0  blocks.2.ffn.key.weight\n",
      "512   512   0    blocks.2.ffn.receptance.weight\n",
      "512   2048  0    blocks.2.ffn.value.weight\n",
      "512   512   0    blocks.3.att.key.weight\n",
      "512   512   1.0  blocks.3.att.value.weight\n",
      "512   512   0    blocks.3.att.receptance.weight\n",
      "512   512   0    blocks.3.att.output.weight\n",
      "2048  512   1.0  blocks.3.ffn.key.weight\n",
      "512   512   0    blocks.3.ffn.receptance.weight\n",
      "512   2048  0    blocks.3.ffn.value.weight\n",
      "512   512   0    blocks.4.att.key.weight\n",
      "512   512   1.0  blocks.4.att.value.weight\n",
      "512   512   0    blocks.4.att.receptance.weight\n",
      "512   512   0    blocks.4.att.output.weight\n",
      "2048  512   1.0  blocks.4.ffn.key.weight\n",
      "512   512   0    blocks.4.ffn.receptance.weight\n",
      "512   2048  0    blocks.4.ffn.value.weight\n",
      "512   512   0    blocks.5.att.key.weight\n",
      "512   512   1.0  blocks.5.att.value.weight\n",
      "512   512   0    blocks.5.att.receptance.weight\n",
      "512   512   0    blocks.5.att.output.weight\n",
      "2048  512   1.0  blocks.5.ffn.key.weight\n",
      "512   512   0    blocks.5.ffn.receptance.weight\n",
      "512   2048  0    blocks.5.ffn.value.weight\n",
      "50277 512   0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "# Lets initialized the L6-D512 model with the init_model.py code\n",
    "!cd ../../RWKV-v4neo/ && python3 init_model.py --n_layer 6 --n_embd 512 --vocab_size neox ../model/L6-D512-neox-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|█| 5318/5318 [00:00<00:00, 327177.24 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 54/54 [00:00<00:00, 30811.10 examples/s\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v4neo && python3 preload_datapath.py ../notebook/trainer-validation/config/baseline-dryrun.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Code validation via dryrun\n",
    "\n",
    "The following dryrun, help do a basic check that the existing trainer code changes are valid across 2 * 2 data samples.\n",
    "\n",
    "If this check fail, its most probably a code / envrionment setup issue (no further checks needed)\n",
    "\n",
    "It does not log the run the W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-26 00:06:34,193] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230725'\n",
      "Global seed set to 3941088705\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu118/wkv_128_bf16/build.ninja...\n",
      "Building extension module wkv_128_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_128_bf16...\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx-nightly/lib/python3.11/site-packages/lightning/fabric/connector.py:554: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx-nightly/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Saving the dataset (1/1 shards): 100%|█| 5318/5318 [00:00<00:00, 228278.38 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 54/54 [00:00<00:00, 22266.26 examples/s\n",
      "[rank: 0] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-26 00:06:47,550] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.2948977947235107 seconds\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|  | 1/5318 [00:09<14:20:48,  9.71s/it, v_num=10, train/loss=10.90]/home/picocreator/anaconda3/envs/rwkv-infctx-nightly/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1690270917945/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
      "Epoch 0:   0%|  | 4/5318 [00:34<12:34:12,  8.52s/it, v_num=10, train/loss=9.750]`Trainer.fit` stopped: `max_steps=2` reached.\n",
      "Epoch 0:   0%|  | 4/5318 [00:34<12:34:13,  8.52s/it, v_num=10, train/loss=9.750]\n"
     ]
    }
   ],
   "source": [
    "# Validate source code and env is working, by doing a short 2 sample dryrun\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit -c ../notebook/trainer-validation/config/baseline-dryrun.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Baseline full context (1024) training\n",
    "\n",
    "Perform a full 1 epoch training run of training context size = 1024. Ensuring all data samples fit within the allocated training size. And is used as the baseline loss comparision for several experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-01 20:53:18,310] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230701_205320-k8flu72z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-validation-full (train-ctx=1024, data-ctx=1024, bs=12)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/k8flu72z\u001b[0m\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 675.41it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3d43d1724bef83d7_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5033407f38c97f24.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-78e7f3a5f1679aa4_*_of_00016.arrow\n",
      "/home/picocreator/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/fabric/connector.py:555: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[rank: 0] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-01 20:53:34,204] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/home/picocreator/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/picocreator/rwkv-proj/infctx-dev/checkpoint/trainer-validaiton/infctx-validation-full exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.312431812286377 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 5318/5318 [51:02<00:00,  1.74it/s, v_num=u72z, train/loss=5.940\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/54 [00:00<00:08,  5.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/54 [00:00<00:07,  6.81it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 3/54 [00:00<00:07,  7.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 4/54 [00:00<00:06,  7.32it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 5/54 [00:00<00:06,  7.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 6/54 [00:00<00:06,  7.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 7/54 [00:00<00:06,  7.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 8/54 [00:01<00:06,  7.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 9/54 [00:01<00:05,  7.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎              | 10/54 [00:01<00:05,  7.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▋              | 11/54 [00:01<00:05,  7.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████              | 12/54 [00:01<00:05,  7.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▎             | 13/54 [00:01<00:05,  7.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▋             | 14/54 [00:01<00:05,  7.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 15/54 [00:01<00:05,  7.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▎            | 16/54 [00:02<00:04,  7.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 17/54 [00:02<00:04,  7.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 18/54 [00:02<00:04,  7.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 19/54 [00:02<00:04,  7.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 20/54 [00:02<00:04,  7.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████           | 21/54 [00:02<00:04,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 22/54 [00:02<00:04,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 23/54 [00:02<00:03,  7.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 24/54 [00:03<00:03,  7.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 25/54 [00:03<00:03,  7.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 26/54 [00:03<00:03,  7.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 27/54 [00:03<00:03,  7.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 28/54 [00:03<00:03,  7.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 29/54 [00:03<00:03,  7.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 30/54 [00:03<00:03,  7.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 31/54 [00:03<00:02,  7.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 32/54 [00:04<00:02,  7.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 33/54 [00:04<00:02,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 34/54 [00:04<00:02,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 35/54 [00:04<00:02,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 36/54 [00:04<00:02,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 37/54 [00:04<00:02,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 38/54 [00:04<00:02,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 39/54 [00:04<00:01,  7.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 40/54 [00:05<00:01,  7.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▋    | 41/54 [00:05<00:01,  7.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 42/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▎   | 43/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 44/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 45/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 46/54 [00:05<00:01,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 47/54 [00:05<00:00,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 48/54 [00:06<00:00,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 49/54 [00:06<00:00,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 50/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 51/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 52/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 53/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 5318/5318 [51:11<00:00,  1.73it/s, v_num=u72z, train/loss=5.940\u001b[A\n",
      "Epoch 0: 100%|█| 5318/5318 [51:11<00:00,  1.73it/s, v_num=u72z, train/loss=5.940`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5318/5318 [51:11<00:00,  1.73it/s, v_num=u72z, train/loss=5.940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ██▇▇█▆▇▆▄▅▄▃▄▄▄▄▂▃▃▃▃▃▃▄▁▂▂▃▃▂▂▂▂▃▂▂▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 5.96875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 5.67332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-validation-full (train-ctx=1024, data-ctx=1024, bs=12)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/k8flu72z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230701_205320-k8flu72z/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Full training run\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit -c ../notebook/trainer-validation/config/baseline-1024.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
