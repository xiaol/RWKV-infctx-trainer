{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto resume checkpoint testing\n",
    "\n",
    "This is to help validate the ability of various commands, to be \"auto-resumed\" from various interruptions.\n",
    "\n",
    "Manual interruption may be needed for several segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets setup the various directories required\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model init, with skip on rerun\n",
    "\n",
    "We initialize a model, and skip it, if file already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-03 07:21:07,341] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_1_bf16/build.ninja...\n",
      "Building extension module wkv_1_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1_bf16...\n",
      "[RWKV.model]: Finished initial model load\n",
      "50277 512   -0.1 emb.weight\n",
      "512   512   0    blocks.0.att.key.weight\n",
      "512   512   1.0  blocks.0.att.value.weight\n",
      "512   512   0    blocks.0.att.receptance.weight\n",
      "512   512   0    blocks.0.att.output.weight\n",
      "2048  512   1.0  blocks.0.ffn.key.weight\n",
      "512   512   0    blocks.0.ffn.receptance.weight\n",
      "512   2048  0    blocks.0.ffn.value.weight\n",
      "512   512   0    blocks.1.att.key.weight\n",
      "512   512   1.0  blocks.1.att.value.weight\n",
      "512   512   0    blocks.1.att.receptance.weight\n",
      "512   512   0    blocks.1.att.output.weight\n",
      "2048  512   1.0  blocks.1.ffn.key.weight\n",
      "512   512   0    blocks.1.ffn.receptance.weight\n",
      "512   2048  0    blocks.1.ffn.value.weight\n",
      "512   512   0    blocks.2.att.key.weight\n",
      "512   512   1.0  blocks.2.att.value.weight\n",
      "512   512   0    blocks.2.att.receptance.weight\n",
      "512   512   0    blocks.2.att.output.weight\n",
      "2048  512   1.0  blocks.2.ffn.key.weight\n",
      "512   512   0    blocks.2.ffn.receptance.weight\n",
      "512   2048  0    blocks.2.ffn.value.weight\n",
      "512   512   0    blocks.3.att.key.weight\n",
      "512   512   1.0  blocks.3.att.value.weight\n",
      "512   512   0    blocks.3.att.receptance.weight\n",
      "512   512   0    blocks.3.att.output.weight\n",
      "2048  512   1.0  blocks.3.ffn.key.weight\n",
      "512   512   0    blocks.3.ffn.receptance.weight\n",
      "512   2048  0    blocks.3.ffn.value.weight\n",
      "512   512   0    blocks.4.att.key.weight\n",
      "512   512   1.0  blocks.4.att.value.weight\n",
      "512   512   0    blocks.4.att.receptance.weight\n",
      "512   512   0    blocks.4.att.output.weight\n",
      "2048  512   1.0  blocks.4.ffn.key.weight\n",
      "512   512   0    blocks.4.ffn.receptance.weight\n",
      "512   2048  0    blocks.4.ffn.value.weight\n",
      "512   512   0    blocks.5.att.key.weight\n",
      "512   512   1.0  blocks.5.att.value.weight\n",
      "512   512   0    blocks.5.att.receptance.weight\n",
      "512   512   0    blocks.5.att.output.weight\n",
      "2048  512   1.0  blocks.5.ffn.key.weight\n",
      "512   512   0    blocks.5.ffn.receptance.weight\n",
      "512   2048  0    blocks.5.ffn.value.weight\n",
      "50277 512   0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "# Lets initialized the L6-D512 model with the init_model.py code\n",
    "# first run should do the full init - if the file does not exists\n",
    "!cd ../../RWKV-v4neo/ && python3 init_model.py --skip-if-exists --n_layer 6 --n_embd 512 --vocab_size neox ../model/L6-D512-neox-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-03 07:21:25,807] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Lets rerun it again, it should skip this time round\n",
    "!cd ../../RWKV-v4neo/ && python3 init_model.py --skip-if-exists --n_layer 6 --n_embd 512 --vocab_size neox ../model/L6-D512-neox-init.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapath setup, cache skip if its done\n",
    "\n",
    "Datapath, already by design, is safely rerunable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v4neo && python3 preload_datapath.py ../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-01e7352a6ba33e03_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-295f4a3681ee7c83_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-aecf54c7557ed3b6_*_of_00032.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets rerun it again! (Quick cache validation!)\n",
    "!cd ../../RWKV-v4neo && python3 preload_datapath.py ../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training run!\n",
    "\n",
    "Everything above should have been done with manual supervision (to make sure its all cleared). The remaining in concept can be let to \"auto run\" / \"auto resume\" on a pre-emptible instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RWKV.lightning_trainer.py]: Running with PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n",
      "[RWKV.lightning_trainer.py] Extracting checkpoint dir from config, for --auto-resume-ckpt-dir=auto\n",
      "[RWKV.lightning_trainer.py] Enabling --auto-resume-ckpt-dir=../checkpoint/trainer-validaiton/ckpt-auto-resume-test --auto-resume-ckpt-mode=2nd-last\n",
      "[RWKV.lightning_trainer.py] No checkpoints found in '../checkpoint/trainer-validaiton/ckpt-auto-resume-test', starting from scratch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-03 07:41:08,278] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml', '--auto-resume-ckpt-dir', 'auto'], args=['fit', '-c', '../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml'].\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1114373010\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1114373010\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230803_074110-aacg83ak\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-validation ckpt-test (train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/aacg83ak\u001b[0m\n",
      "[RWKV.model]: Preloading model from '../model/L6-D512-neox-init.pth'\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=1024 -c /home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    12288 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=1024 -c /home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_1024_bf16.so\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[RWKV.model]: Loading model weights ( L6-D512-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 902.00it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-01e7352a6ba33e03_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-295f4a3681ee7c83_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-aecf54c7557ed3b6_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 1114373010                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-03 07:41:34,722] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -c /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 29.777660369873047 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "[RWKV.model][rank=0] Loaded optimizer (linear schedule) ...\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(71960576, False), (3072, False), (3072, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Epoch 0:   0%|  | 15/5308 [00:06<37:25,  2.36it/s, v_num=83ak, train/loss=10.90]/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1688627653114/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
      "Epoch 0:   1%|  | 32/5308 [00:08<23:27,  3.75it/s, v_num=83ak, train/loss=10.20]/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1825: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0:   2%| | 128/5308 [00:25<17:02,  5.07it/s, v_num=83ak, train/loss=8.440]^C\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Lets start a simple run, interrupt this after 32+ steps, where it should create a checkpoint\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit \\\n",
    "    -c ../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml \\\n",
    "    --auto-resume-ckpt-dir \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RWKV.lightning_trainer.py] Running with PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n",
      "[RWKV.lightning_trainer.py] Extracting checkpoint dir from config, for --auto-resume-ckpt-dir=auto\n",
      "[RWKV.lightning_trainer.py] Enabling --auto-resume-ckpt-dir=../checkpoint/trainer-validaiton/ckpt-auto-resume-test --auto-resume-ckpt-mode=2nd-last\n",
      "[RWKV.lightning_trainer.py] Found 5 checkpoints in '../checkpoint/trainer-validaiton/ckpt-auto-resume-test', using 'last.ckpt'\n",
      "[2023-08-03 07:43:47,587] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml', '--auto-resume-ckpt-dir', 'auto'], args=['fit', '-c', '../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml', '--ckpt_path', '../checkpoint/trainer-validaiton/ckpt-auto-resume-test/last.ckpt'].\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 636907093\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 636907093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230803_074349-aacg83ak\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33minfctx-validation ckpt-test (train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/aacg83ak\u001b[0m\n",
      "[RWKV.model]: Preloading model from '../model/L6-D512-neox-init.pth'\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[RWKV.model]: Loading model weights ( L6-D512-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 923.25it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-01e7352a6ba33e03_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-295f4a3681ee7c83_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-aecf54c7557ed3b6_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 636907093                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-03 07:43:51,527] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/ubuntu/picocreator-memory-experiment/checkpoint/trainer-validaiton/ckpt-auto-resume-test exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3316457271575928 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "[RWKV.model][rank=0] Loaded optimizer (linear schedule) ...\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(71960576, False), (3072, False), (3072, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Restoring states from the checkpoint path at ../checkpoint/trainer-validaiton/ckpt-auto-resume-test/last.ckpt\n",
      "Restored all states from the checkpoint at ../checkpoint/trainer-validaiton/ckpt-auto-resume-test/last.ckpt\n",
      "Epoch 0:   0%|                                         | 0/5308 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:151: UserWarning: You're resuming from a checkpoint that ended before the epoch ended. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint\n",
      "  rank_zero_warn(\n",
      "Epoch 0:   2%| | 110/5308 [00:06<05:09, 16.82it/s, v_num=83ak, train/loss=9.000]/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1688627653114/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
      "Epoch 0:   2%| | 112/5308 [00:07<06:00, 14.40it/s, v_num=83ak, train/loss=8.750]/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1825: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0:  14%|▏| 752/5308 [02:19<14:03,  5.40it/s, v_num=83ak, train/loss=7.120]^C\n"
     ]
    }
   ],
   "source": [
    "# Lets run it again, and confirm if it resumed from the checkpoint\n",
    "# Lets interrupt this again at a later stage\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit \\\n",
    "    -c ../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml \\\n",
    "    --auto-resume-ckpt-dir \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RWKV.lightning_trainer.py] Running with PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n",
      "[RWKV.lightning_trainer.py] Extracting checkpoint dir from config, for --auto-resume-ckpt-dir=auto\n",
      "[RWKV.lightning_trainer.py] Enabling --auto-resume-ckpt-dir=../checkpoint/trainer-validaiton/ckpt-auto-resume-test --auto-resume-ckpt-mode=2nd-last\n",
      "[RWKV.lightning_trainer.py] Found 5 checkpoints in '../checkpoint/trainer-validaiton/ckpt-auto-resume-test', using 'last.ckpt'\n",
      "[2023-08-03 07:46:48,809] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml', '--auto-resume-ckpt-dir', 'auto'], args=['fit', '-c', '../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml', '--ckpt_path', '../checkpoint/trainer-validaiton/ckpt-auto-resume-test/last.ckpt'].\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 655468113\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 655468113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230803_074650-aacg83ak\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33minfctx-validation ckpt-test (train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/aacg83ak\u001b[0m\n",
      "[RWKV.model]: Preloading model from '../model/L6-D512-neox-init.pth'\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[RWKV.model]: Loading model weights ( L6-D512-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 906.09it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-01e7352a6ba33e03_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-295f4a3681ee7c83_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-aecf54c7557ed3b6_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 655468113                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-03 07:46:52,809] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/ubuntu/picocreator-memory-experiment/checkpoint/trainer-validaiton/ckpt-auto-resume-test exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.331468343734741 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "[RWKV.model][rank=0] Loaded optimizer (linear schedule) ...\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(71960576, False), (3072, False), (3072, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Restoring states from the checkpoint path at ../checkpoint/trainer-validaiton/ckpt-auto-resume-test/last.ckpt\n",
      "Restored all states from the checkpoint at ../checkpoint/trainer-validaiton/ckpt-auto-resume-test/last.ckpt\n",
      "Epoch 0:   0%|                                         | 0/5308 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:151: UserWarning: You're resuming from a checkpoint that ended before the epoch ended. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  14%|▏| 766/5308 [00:06<00:37, 120.02it/s, v_num=83ak, train/loss=7.090/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1688627653114/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
      "Epoch 0:  14%|▏| 768/5308 [00:07<00:45, 100.84it/s, v_num=83ak, train/loss=7.340/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1825: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 5308/5308 [15:40<00:00,  5.64it/s, v_num=83ak, train/loss=5.660\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 1/54 [00:00<00:00, 261.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/54 [00:00<00:00, 86.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 3/54 [00:00<00:00, 123.38it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                | 4/54 [00:00<00:00, 156.49it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▋                | 5/54 [00:00<00:00, 186.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                | 6/54 [00:00<00:00, 213.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎               | 7/54 [00:00<00:00, 239.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▋               | 8/54 [00:00<00:00, 187.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███               | 9/54 [00:00<00:00, 204.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 10/54 [00:00<00:00, 221.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 11/54 [00:00<00:00, 236.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 12/54 [00:00<00:00, 251.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 13/54 [00:00<00:00, 266.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 14/54 [00:00<00:00, 279.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▋            | 15/54 [00:00<00:00, 292.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 16/54 [00:00<00:00, 304.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 17/54 [00:00<00:00, 316.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 18/54 [00:00<00:00, 327.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 19/54 [00:00<00:00, 338.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 20/54 [00:00<00:00, 341.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 21/54 [00:00<00:00, 351.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 22/54 [00:00<00:00, 360.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▏         | 23/54 [00:00<00:00, 369.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▌         | 24/54 [00:00<00:00, 378.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 25/54 [00:00<00:00, 387.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 26/54 [00:00<00:00, 395.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 27/54 [00:00<00:00, 403.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 28/54 [00:00<00:00, 411.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 29/54 [00:00<00:00, 418.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▍       | 30/54 [00:00<00:00, 425.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▊       | 31/54 [00:00<00:00, 432.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 32/54 [00:00<00:00, 439.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 33/54 [00:00<00:00, 446.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 34/54 [00:00<00:00, 452.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████      | 35/54 [00:00<00:00, 458.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|███████████▎     | 36/54 [00:00<00:00, 464.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████▋     | 37/54 [00:00<00:00, 470.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▉     | 38/54 [00:00<00:00, 476.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▎    | 39/54 [00:00<00:00, 481.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|████████████▌    | 40/54 [00:00<00:00, 479.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▉    | 41/54 [00:00<00:00, 485.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|█████████████▏   | 42/54 [00:00<00:00, 490.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|█████████████▌   | 43/54 [00:00<00:00, 495.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|█████████████▊   | 44/54 [00:00<00:00, 500.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▏  | 45/54 [00:00<00:00, 505.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|██████████████▍  | 46/54 [00:00<00:00, 509.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|██████████████▊  | 47/54 [00:00<00:00, 514.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|███████████████  | 48/54 [00:00<00:00, 518.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|███████████████▍ | 49/54 [00:00<00:00, 522.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|███████████████▋ | 50/54 [00:00<00:00, 526.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████ | 51/54 [00:00<00:00, 531.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|████████████████▎| 52/54 [00:00<00:00, 528.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|████████████████▋| 53/54 [00:00<00:00, 532.63it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [15:43<00:00,  5.63it/s, v_num=83ak, train/loss=5.660\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [15:43<00:00,  5.63it/s, v_num=83ak, train/loss=5.660\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5308/5308 [15:46<00:00,  5.61it/s, v_num=83ak, train/loss=5.660\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▇▇█▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▅█▇▇▇▇▆▇▇▆▇▇▇▆▇▆▇▆▆▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 331\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-validation ckpt-test (train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/aacg83ak\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230803_074650-aacg83ak/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Lets run it again, and let it run to completion\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit \\\n",
    "    -c ../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml \\\n",
    "    --auto-resume-ckpt-dir \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RWKV.lightning_trainer.py] Running with PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n",
      "[RWKV.lightning_trainer.py] Extracting checkpoint dir from config, for --auto-resume-ckpt-dir=auto\n",
      "[RWKV.lightning_trainer.py] Enabling --auto-resume-ckpt-dir=../checkpoint/trainer-validaiton/ckpt-auto-resume-test --auto-resume-ckpt-mode=2nd-last\n",
      "[RWKV.lightning_trainer.py] Found 5 checkpoints in '../checkpoint/trainer-validaiton/ckpt-auto-resume-test', using 'epoch=0-step=332.ckpt'\n",
      "[2023-08-03 08:09:00,684] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml', '--auto-resume-ckpt-dir', 'auto'], args=['fit', '-c', '../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml', '--ckpt_path', '../checkpoint/trainer-validaiton/ckpt-auto-resume-test/epoch=0-step=332.ckpt'].\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 663606280\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 663606280\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230803_080902-aacg83ak\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33minfctx-validation ckpt-test (train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/aacg83ak\u001b[0m\n",
      "[RWKV.model]: Preloading model from '../model/L6-D512-neox-init.pth'\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[RWKV.model]: Loading model weights ( L6-D512-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 932.48it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-01e7352a6ba33e03_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-295f4a3681ee7c83_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-aecf54c7557ed3b6_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 663606280                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-03 08:09:04,573] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/ubuntu/picocreator-memory-experiment/checkpoint/trainer-validaiton/ckpt-auto-resume-test exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.327439069747925 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "[RWKV.model][rank=0] Loaded optimizer (linear schedule) ...\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(71960576, False), (3072, False), (3072, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Restoring states from the checkpoint path at ../checkpoint/trainer-validaiton/ckpt-auto-resume-test/epoch=0-step=332.ckpt\n",
      "Restored all states from the checkpoint at ../checkpoint/trainer-validaiton/ckpt-auto-resume-test/epoch=0-step=332.ckpt\n",
      "Epoch 0: 100%|▉| 5294/5308 [00:06<00:00, 824.47it/s, v_num=83ak, train/loss=5.53/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1688627653114/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
      "Epoch 0: 100%|▉| 5296/5308 [00:07<00:00, 693.02it/s, v_num=83ak, train/loss=5.38/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1825: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 5308/5308 [00:12<00:00, 408.58it/s, v_num=83ak, train/loss=5.56\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 1/54 [00:00<00:00, 250.63it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 2/54 [00:00<00:00, 361.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 3/54 [00:00<00:00, 432.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                | 4/54 [00:00<00:00, 474.16it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▋                | 5/54 [00:00<00:00, 513.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                | 6/54 [00:00<00:00, 534.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎               | 7/54 [00:00<00:00, 551.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▋               | 8/54 [00:00<00:00, 569.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███               | 9/54 [00:00<00:00, 581.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 10/54 [00:00<00:00, 593.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 11/54 [00:00<00:00, 601.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 12/54 [00:00<00:00, 611.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 13/54 [00:00<00:00, 619.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 14/54 [00:00<00:00, 626.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▋            | 15/54 [00:00<00:00, 629.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 16/54 [00:00<00:00, 636.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 17/54 [00:00<00:00, 642.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 18/54 [00:00<00:00, 646.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 19/54 [00:00<00:00, 651.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 20/54 [00:00<00:00, 655.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 21/54 [00:00<00:00, 658.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 22/54 [00:00<00:00, 661.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▏         | 23/54 [00:00<00:00, 664.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▌         | 24/54 [00:00<00:00, 665.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 25/54 [00:00<00:00, 668.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 26/54 [00:00<00:00, 671.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 27/54 [00:00<00:00, 673.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 28/54 [00:00<00:00, 675.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 29/54 [00:00<00:00, 677.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▍       | 30/54 [00:00<00:00, 660.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▊       | 31/54 [00:00<00:00, 662.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 32/54 [00:00<00:00, 664.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 33/54 [00:00<00:00, 655.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 34/54 [00:00<00:00, 656.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████      | 35/54 [00:00<00:00, 658.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|███████████▎     | 36/54 [00:00<00:00, 660.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████▋     | 37/54 [00:00<00:00, 661.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▉     | 38/54 [00:00<00:00, 662.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▎    | 39/54 [00:00<00:00, 664.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|████████████▌    | 40/54 [00:00<00:00, 666.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▉    | 41/54 [00:00<00:00, 666.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|█████████████▏   | 42/54 [00:00<00:00, 667.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|█████████████▌   | 43/54 [00:00<00:00, 669.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|█████████████▊   | 44/54 [00:00<00:00, 670.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▏  | 45/54 [00:00<00:00, 672.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|██████████████▍  | 46/54 [00:00<00:00, 676.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|██████████████▊  | 47/54 [00:00<00:00, 679.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|███████████████  | 48/54 [00:00<00:00, 683.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|███████████████▍ | 49/54 [00:00<00:00, 686.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|███████████████▋ | 50/54 [00:00<00:00, 689.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████ | 51/54 [00:00<00:00, 692.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|████████████████▎| 52/54 [00:00<00:00, 695.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|████████████████▋| 53/54 [00:00<00:00, 698.34it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [00:15<00:00, 339.65it/s, v_num=83ak, train/loss=5.56\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [00:15<00:00, 339.63it/s, v_num=83ak, train/loss=5.56`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5308/5308 [00:15<00:00, 339.61it/s, v_num=83ak, train/loss=5.56\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ██████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ██████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▇▇▇▇▇█▇▇▇▇▇▇██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▃▆▆▆▆▆▆▆███████████████████████████████▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 331\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-validation ckpt-test (train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/aacg83ak\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230803_080902-aacg83ak/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Lets see if there is any issues when rerunning after completion\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit \\\n",
    "    -c ../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml \\\n",
    "    --auto-resume-ckpt-dir \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RWKV.lightning_trainer.py] Running with PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n",
      "[RWKV.lightning_trainer.py] Extracting checkpoint dir from config, for --auto-resume-ckpt-dir=auto\n",
      "[RWKV.lightning_trainer.py] Enabling --auto-resume-ckpt-dir=../checkpoint/trainer-validaiton/ckpt-auto-resume-test --auto-resume-ckpt-mode=last\n",
      "[RWKV.lightning_trainer.py] Found 5 checkpoints in '../checkpoint/trainer-validaiton/ckpt-auto-resume-test', using 'last.ckpt'\n",
      "[RWKV.lightning_trainer.py][warning] Pytorch Lightning timing estimates can be very inaccurate when resuming from a checkpoint, due to the way it calculates the time left. See: https://github.com/Lightning-AI/lightning/issues/18220\n",
      "[2023-08-03 08:17:08,832] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml', '--auto-resume-ckpt-dir', 'auto', '--auto-resume-ckpt-mode', 'last'], args=['fit', '-c', '../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml', '--ckpt_path', '../checkpoint/trainer-validaiton/ckpt-auto-resume-test/last.ckpt'].\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 4260174562\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 4260174562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230803_081710-aacg83ak\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33minfctx-validation ckpt-test (train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/aacg83ak\u001b[0m\n",
      "[RWKV.model]: Preloading model from '../model/L6-D512-neox-init.pth'\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[RWKV.model]: Loading model weights ( L6-D512-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 902.58it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-01e7352a6ba33e03_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-295f4a3681ee7c83_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-aecf54c7557ed3b6_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 4260174562                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-03 08:17:12,630] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/ubuntu/picocreator-memory-experiment/checkpoint/trainer-validaiton/ckpt-auto-resume-test exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.33762788772583 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "[RWKV.model][rank=0] Loaded optimizer (linear schedule) ...\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(71960576, False), (3072, False), (3072, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Restoring states from the checkpoint path at ../checkpoint/trainer-validaiton/ckpt-auto-resume-test/last.ckpt\n",
      "Restored all states from the checkpoint at ../checkpoint/trainer-validaiton/ckpt-auto-resume-test/last.ckpt\n",
      "Epoch 0:   0%|                                         | 0/5308 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:151: UserWarning: You're resuming from a checkpoint that ended before the epoch ended. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 5308/5308 [00:06<00:00, 834.62it/s, v_num=83ak, train/loss=5.19\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 1/54 [00:00<00:00, 273.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 2/54 [00:00<00:00, 385.72it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 3/54 [00:00<00:00, 463.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                | 4/54 [00:00<00:00, 512.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▋                | 5/54 [00:00<00:00, 548.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                | 6/54 [00:00<00:00, 578.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎               | 7/54 [00:00<00:00, 602.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▋               | 8/54 [00:00<00:00, 619.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███               | 9/54 [00:00<00:00, 637.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 10/54 [00:00<00:00, 653.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 11/54 [00:00<00:00, 660.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 12/54 [00:00<00:00, 674.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 13/54 [00:00<00:00, 688.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 14/54 [00:00<00:00, 693.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▋            | 15/54 [00:00<00:00, 702.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 16/54 [00:00<00:00, 712.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 17/54 [00:00<00:00, 718.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 18/54 [00:00<00:00, 722.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 19/54 [00:00<00:00, 727.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 20/54 [00:00<00:00, 730.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 21/54 [00:00<00:00, 732.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 22/54 [00:00<00:00, 736.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▏         | 23/54 [00:00<00:00, 688.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▌         | 24/54 [00:00<00:00, 693.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 25/54 [00:00<00:00, 696.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 26/54 [00:00<00:00, 700.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 27/54 [00:00<00:00, 704.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 28/54 [00:00<00:00, 708.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 29/54 [00:00<00:00, 711.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▍       | 30/54 [00:00<00:00, 714.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▊       | 31/54 [00:00<00:00, 719.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 32/54 [00:00<00:00, 721.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 33/54 [00:00<00:00, 723.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 34/54 [00:00<00:00, 726.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████      | 35/54 [00:00<00:00, 727.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|███████████▎     | 36/54 [00:00<00:00, 730.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████▋     | 37/54 [00:00<00:00, 733.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▉     | 38/54 [00:00<00:00, 734.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▎    | 39/54 [00:00<00:00, 736.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|████████████▌    | 40/54 [00:00<00:00, 739.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▉    | 41/54 [00:00<00:00, 740.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|█████████████▏   | 42/54 [00:00<00:00, 742.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|█████████████▌   | 43/54 [00:00<00:00, 745.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|█████████████▊   | 44/54 [00:00<00:00, 747.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▏  | 45/54 [00:00<00:00, 747.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|██████████████▍  | 46/54 [00:00<00:00, 749.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|██████████████▊  | 47/54 [00:00<00:00, 751.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|███████████████  | 48/54 [00:00<00:00, 753.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|███████████████▍ | 49/54 [00:00<00:00, 754.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|███████████████▋ | 50/54 [00:00<00:00, 756.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████ | 51/54 [00:00<00:00, 758.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|████████████████▎| 52/54 [00:00<00:00, 760.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|████████████████▋| 53/54 [00:00<00:00, 758.75it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [00:08<00:00, 595.17it/s, v_num=83ak, train/loss=5.19\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [00:08<00:00, 595.09it/s, v_num=83ak, train/loss=5.19/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1825: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5308/5308 [00:12<00:00, 427.47it/s, v_num=83ak, train/loss=5.19\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▇▇▆▇▇██▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▆██████████████████████████████████████▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 331\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-validation ckpt-test (train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/aacg83ak\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230803_081710-aacg83ak/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The downside of using \"2nd last\" is that the `save_on_train_epoch_end: true` flag will only update the last.ckpt\n",
    "# meaning we may end up retraining the last few steps repeatingly, if not handled carefully in notebooks. \n",
    "#\n",
    "# In most cases, this is not an isssue, but you can trade in resiliance protection, for truely loading the last checkpoint instead if needed\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit \\\n",
    "    -c ../notebook/trainer-validation/config/ckpt-auto-resume-test-1024.yaml \\\n",
    "    --auto-resume-ckpt-dir \"auto\" \\\n",
    "    --auto-resume-ckpt-mode \"last\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
