{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the trainer code, against various model sizes\n",
    "\n",
    "This helps validate the RWKV trainer can run against various sizes.\n",
    "\n",
    "This does not perform any actual checkpointing / export (yet?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets setup the various directories required\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU_DEVICES: auto\n",
      "DEEPSPEED_STRAT: deepspeed_stage_3\n",
      "NOTEBOOK_DIR: /root/picocreator-dev-infctx/notebook/trainer-validation\n",
      "TRAINER_DIR: /root/picocreator-dev-infctx/RWKV-v4neo\n",
      "PROJECT_DIR: /root/picocreator-dev-infctx\n"
     ]
    }
   ],
   "source": [
    "# Confgiure the deepspeed / gpu count to be tested\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_3\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 779.90it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-1062ba7da82e617a_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-49681c8aebfc1cc9_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-47774371f91aea93_*_of_00064.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/minimal-enwiki.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5B Size (L24-D2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "---- Initializing model ----\n",
      "No of layers: 24\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L24-D2048-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_2048_bf16/build.ninja...\n",
      "Building extension module wkv_2048_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_2048_bf16...\n",
      "[RWKV.model]: Finished initial model load\n",
      "50277 2048  -0.1 emb.weight\n",
      "2048  2048  0    blocks.0.att.key.weight\n",
      "2048  2048  1.0  blocks.0.att.value.weight\n",
      "2048  2048  0    blocks.0.att.receptance.weight\n",
      "2048  2048  0    blocks.0.att.output.weight\n",
      "8192  2048  1.0  blocks.0.ffn.key.weight\n",
      "2048  2048  0    blocks.0.ffn.receptance.weight\n",
      "2048  8192  0    blocks.0.ffn.value.weight\n",
      "2048  2048  0    blocks.1.att.key.weight\n",
      "2048  2048  1.0  blocks.1.att.value.weight\n",
      "2048  2048  0    blocks.1.att.receptance.weight\n",
      "2048  2048  0    blocks.1.att.output.weight\n",
      "8192  2048  1.0  blocks.1.ffn.key.weight\n",
      "2048  2048  0    blocks.1.ffn.receptance.weight\n",
      "2048  8192  0    blocks.1.ffn.value.weight\n",
      "2048  2048  0    blocks.2.att.key.weight\n",
      "2048  2048  1.0  blocks.2.att.value.weight\n",
      "2048  2048  0    blocks.2.att.receptance.weight\n",
      "2048  2048  0    blocks.2.att.output.weight\n",
      "8192  2048  1.0  blocks.2.ffn.key.weight\n",
      "2048  2048  0    blocks.2.ffn.receptance.weight\n",
      "2048  8192  0    blocks.2.ffn.value.weight\n",
      "2048  2048  0    blocks.3.att.key.weight\n",
      "2048  2048  1.0  blocks.3.att.value.weight\n",
      "2048  2048  0    blocks.3.att.receptance.weight\n",
      "2048  2048  0    blocks.3.att.output.weight\n",
      "8192  2048  1.0  blocks.3.ffn.key.weight\n",
      "2048  2048  0    blocks.3.ffn.receptance.weight\n",
      "2048  8192  0    blocks.3.ffn.value.weight\n",
      "2048  2048  0    blocks.4.att.key.weight\n",
      "2048  2048  1.0  blocks.4.att.value.weight\n",
      "2048  2048  0    blocks.4.att.receptance.weight\n",
      "2048  2048  0    blocks.4.att.output.weight\n",
      "8192  2048  1.0  blocks.4.ffn.key.weight\n",
      "2048  2048  0    blocks.4.ffn.receptance.weight\n",
      "2048  8192  0    blocks.4.ffn.value.weight\n",
      "2048  2048  0    blocks.5.att.key.weight\n",
      "2048  2048  1.0  blocks.5.att.value.weight\n",
      "2048  2048  0    blocks.5.att.receptance.weight\n",
      "2048  2048  0    blocks.5.att.output.weight\n",
      "8192  2048  1.0  blocks.5.ffn.key.weight\n",
      "2048  2048  0    blocks.5.ffn.receptance.weight\n",
      "2048  8192  0    blocks.5.ffn.value.weight\n",
      "2048  2048  0    blocks.6.att.key.weight\n",
      "2048  2048  1.0  blocks.6.att.value.weight\n",
      "2048  2048  0    blocks.6.att.receptance.weight\n",
      "2048  2048  0    blocks.6.att.output.weight\n",
      "8192  2048  1.0  blocks.6.ffn.key.weight\n",
      "2048  2048  0    blocks.6.ffn.receptance.weight\n",
      "2048  8192  0    blocks.6.ffn.value.weight\n",
      "2048  2048  0    blocks.7.att.key.weight\n",
      "2048  2048  1.0  blocks.7.att.value.weight\n",
      "2048  2048  0    blocks.7.att.receptance.weight\n",
      "2048  2048  0    blocks.7.att.output.weight\n",
      "8192  2048  1.0  blocks.7.ffn.key.weight\n",
      "2048  2048  0    blocks.7.ffn.receptance.weight\n",
      "2048  8192  0    blocks.7.ffn.value.weight\n",
      "2048  2048  0    blocks.8.att.key.weight\n",
      "2048  2048  1.0  blocks.8.att.value.weight\n",
      "2048  2048  0    blocks.8.att.receptance.weight\n",
      "2048  2048  0    blocks.8.att.output.weight\n",
      "8192  2048  1.0  blocks.8.ffn.key.weight\n",
      "2048  2048  0    blocks.8.ffn.receptance.weight\n",
      "2048  8192  0    blocks.8.ffn.value.weight\n",
      "2048  2048  0    blocks.9.att.key.weight\n",
      "2048  2048  1.0  blocks.9.att.value.weight\n",
      "2048  2048  0    blocks.9.att.receptance.weight\n",
      "2048  2048  0    blocks.9.att.output.weight\n",
      "8192  2048  1.0  blocks.9.ffn.key.weight\n",
      "2048  2048  0    blocks.9.ffn.receptance.weight\n",
      "2048  8192  0    blocks.9.ffn.value.weight\n",
      "2048  2048  0    blocks.10.att.key.weight\n",
      "2048  2048  1.0  blocks.10.att.value.weight\n",
      "2048  2048  0    blocks.10.att.receptance.weight\n",
      "2048  2048  0    blocks.10.att.output.weight\n",
      "8192  2048  1.0  blocks.10.ffn.key.weight\n",
      "2048  2048  0    blocks.10.ffn.receptance.weight\n",
      "2048  8192  0    blocks.10.ffn.value.weight\n",
      "2048  2048  0    blocks.11.att.key.weight\n",
      "2048  2048  1.0  blocks.11.att.value.weight\n",
      "2048  2048  0    blocks.11.att.receptance.weight\n",
      "2048  2048  0    blocks.11.att.output.weight\n",
      "8192  2048  1.0  blocks.11.ffn.key.weight\n",
      "2048  2048  0    blocks.11.ffn.receptance.weight\n",
      "2048  8192  0    blocks.11.ffn.value.weight\n",
      "2048  2048  0    blocks.12.att.key.weight\n",
      "2048  2048  1.0  blocks.12.att.value.weight\n",
      "2048  2048  0    blocks.12.att.receptance.weight\n",
      "2048  2048  0    blocks.12.att.output.weight\n",
      "8192  2048  1.0  blocks.12.ffn.key.weight\n",
      "2048  2048  0    blocks.12.ffn.receptance.weight\n",
      "2048  8192  0    blocks.12.ffn.value.weight\n",
      "2048  2048  0    blocks.13.att.key.weight\n",
      "2048  2048  1.0  blocks.13.att.value.weight\n",
      "2048  2048  0    blocks.13.att.receptance.weight\n",
      "2048  2048  0    blocks.13.att.output.weight\n",
      "8192  2048  1.0  blocks.13.ffn.key.weight\n",
      "2048  2048  0    blocks.13.ffn.receptance.weight\n",
      "2048  8192  0    blocks.13.ffn.value.weight\n",
      "2048  2048  0    blocks.14.att.key.weight\n",
      "2048  2048  1.0  blocks.14.att.value.weight\n",
      "2048  2048  0    blocks.14.att.receptance.weight\n",
      "2048  2048  0    blocks.14.att.output.weight\n",
      "8192  2048  1.0  blocks.14.ffn.key.weight\n",
      "2048  2048  0    blocks.14.ffn.receptance.weight\n",
      "2048  8192  0    blocks.14.ffn.value.weight\n",
      "2048  2048  0    blocks.15.att.key.weight\n",
      "2048  2048  1.0  blocks.15.att.value.weight\n",
      "2048  2048  0    blocks.15.att.receptance.weight\n",
      "2048  2048  0    blocks.15.att.output.weight\n",
      "8192  2048  1.0  blocks.15.ffn.key.weight\n",
      "2048  2048  0    blocks.15.ffn.receptance.weight\n",
      "2048  8192  0    blocks.15.ffn.value.weight\n",
      "2048  2048  0    blocks.16.att.key.weight\n",
      "2048  2048  1.0  blocks.16.att.value.weight\n",
      "2048  2048  0    blocks.16.att.receptance.weight\n",
      "2048  2048  0    blocks.16.att.output.weight\n",
      "8192  2048  1.0  blocks.16.ffn.key.weight\n",
      "2048  2048  0    blocks.16.ffn.receptance.weight\n",
      "2048  8192  0    blocks.16.ffn.value.weight\n",
      "2048  2048  0    blocks.17.att.key.weight\n",
      "2048  2048  1.0  blocks.17.att.value.weight\n",
      "2048  2048  0    blocks.17.att.receptance.weight\n",
      "2048  2048  0    blocks.17.att.output.weight\n",
      "8192  2048  1.0  blocks.17.ffn.key.weight\n",
      "2048  2048  0    blocks.17.ffn.receptance.weight\n",
      "2048  8192  0    blocks.17.ffn.value.weight\n",
      "2048  2048  0    blocks.18.att.key.weight\n",
      "2048  2048  1.0  blocks.18.att.value.weight\n",
      "2048  2048  0    blocks.18.att.receptance.weight\n",
      "2048  2048  0    blocks.18.att.output.weight\n",
      "8192  2048  1.0  blocks.18.ffn.key.weight\n",
      "2048  2048  0    blocks.18.ffn.receptance.weight\n",
      "2048  8192  0    blocks.18.ffn.value.weight\n",
      "2048  2048  0    blocks.19.att.key.weight\n",
      "2048  2048  1.0  blocks.19.att.value.weight\n",
      "2048  2048  0    blocks.19.att.receptance.weight\n",
      "2048  2048  0    blocks.19.att.output.weight\n",
      "8192  2048  1.0  blocks.19.ffn.key.weight\n",
      "2048  2048  0    blocks.19.ffn.receptance.weight\n",
      "2048  8192  0    blocks.19.ffn.value.weight\n",
      "2048  2048  0    blocks.20.att.key.weight\n",
      "2048  2048  1.0  blocks.20.att.value.weight\n",
      "2048  2048  0    blocks.20.att.receptance.weight\n",
      "2048  2048  0    blocks.20.att.output.weight\n",
      "8192  2048  1.0  blocks.20.ffn.key.weight\n",
      "2048  2048  0    blocks.20.ffn.receptance.weight\n",
      "2048  8192  0    blocks.20.ffn.value.weight\n",
      "2048  2048  0    blocks.21.att.key.weight\n",
      "2048  2048  1.0  blocks.21.att.value.weight\n",
      "2048  2048  0    blocks.21.att.receptance.weight\n",
      "2048  2048  0    blocks.21.att.output.weight\n",
      "8192  2048  1.0  blocks.21.ffn.key.weight\n",
      "2048  2048  0    blocks.21.ffn.receptance.weight\n",
      "2048  8192  0    blocks.21.ffn.value.weight\n",
      "2048  2048  0    blocks.22.att.key.weight\n",
      "2048  2048  1.0  blocks.22.att.value.weight\n",
      "2048  2048  0    blocks.22.att.receptance.weight\n",
      "2048  2048  0    blocks.22.att.output.weight\n",
      "8192  2048  1.0  blocks.22.ffn.key.weight\n",
      "2048  2048  0    blocks.22.ffn.receptance.weight\n",
      "2048  8192  0    blocks.22.ffn.value.weight\n",
      "2048  2048  0    blocks.23.att.key.weight\n",
      "2048  2048  1.0  blocks.23.att.value.weight\n",
      "2048  2048  0    blocks.23.att.receptance.weight\n",
      "2048  2048  0    blocks.23.att.output.weight\n",
      "8192  2048  1.0  blocks.23.ffn.key.weight\n",
      "2048  2048  0    blocks.23.ffn.receptance.weight\n",
      "2048  8192  0    blocks.23.ffn.value.weight\n",
      "50277 2048  0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 init_model.py \\\n",
    "        --n_layer 24 --n_embd 2048 \\\n",
    "        --vocab_size neox \\\n",
    "        ../model/L24-D2048-neox-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3061363964\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3061363964\n",
      "[RWKV.model]: Preloading model from '../model/L24-D2048-neox-init.pth'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[RWKV.model]: Loading model weights ( L24-D2048-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 807.06it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-1062ba7da82e617a_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-49681c8aebfc1cc9_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-47774371f91aea93_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 3061363964                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-08-01 14:55:34,564] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 3061363964\n",
      "[RWKV.model]: Preloading model from '../model/L24-D2048-neox-init.pth'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[RWKV.model]: Loading model weights ( L24-D2048-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "[rank: 1] Global seed set to 3061363964\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-08-01 14:55:51,774] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[RWKV.model][rank=1] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3830833435058594 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.448824405670166 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.08124637603759766 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10205292701721191 seconds\n",
      "Rank: 1 partition count [2, 2, 2] and sizes[(757504000, False), (24576, False), (24576, False)] \n",
      "Rank: 0 partition count [2, 2, 2] and sizes[(757504000, False), (24576, False), (24576, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004520416259765625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027370452880859375 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|   | 10/2648 [00:57<4:14:29,  5.79s/it, v_num=1, train/loss=8.620]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_steps=10` reached.\n",
      "Epoch 0:   0%|   | 10/2648 [01:06<4:54:18,  6.69s/it, v_num=1, train/loss=8.620]\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/minimal-enwiki.yaml\" \\\n",
    "        --model.load_model \"../model/L24-D2048-neox-init.pth\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B Size (L32-D2560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "---- Initializing model ----\n",
      "No of layers: 32\n",
      "Embedding size: 2560\n",
      "Output model path: ../model/L32-D2560-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_2048_bf16/build.ninja...\n",
      "Building extension module wkv_2048_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_2048_bf16...\n",
      "[RWKV.model]: Finished initial model load\n",
      "50277 2560  -0.1 emb.weight\n",
      "2560  2560  0    blocks.0.att.key.weight\n",
      "2560  2560  1.0  blocks.0.att.value.weight\n",
      "2560  2560  0    blocks.0.att.receptance.weight\n",
      "2560  2560  0    blocks.0.att.output.weight\n",
      "10240 2560  1.0  blocks.0.ffn.key.weight\n",
      "2560  2560  0    blocks.0.ffn.receptance.weight\n",
      "2560  10240 0    blocks.0.ffn.value.weight\n",
      "2560  2560  0    blocks.1.att.key.weight\n",
      "2560  2560  1.0  blocks.1.att.value.weight\n",
      "2560  2560  0    blocks.1.att.receptance.weight\n",
      "2560  2560  0    blocks.1.att.output.weight\n",
      "10240 2560  1.0  blocks.1.ffn.key.weight\n",
      "2560  2560  0    blocks.1.ffn.receptance.weight\n",
      "2560  10240 0    blocks.1.ffn.value.weight\n",
      "2560  2560  0    blocks.2.att.key.weight\n",
      "2560  2560  1.0  blocks.2.att.value.weight\n",
      "2560  2560  0    blocks.2.att.receptance.weight\n",
      "2560  2560  0    blocks.2.att.output.weight\n",
      "10240 2560  1.0  blocks.2.ffn.key.weight\n",
      "2560  2560  0    blocks.2.ffn.receptance.weight\n",
      "2560  10240 0    blocks.2.ffn.value.weight\n",
      "2560  2560  0    blocks.3.att.key.weight\n",
      "2560  2560  1.0  blocks.3.att.value.weight\n",
      "2560  2560  0    blocks.3.att.receptance.weight\n",
      "2560  2560  0    blocks.3.att.output.weight\n",
      "10240 2560  1.0  blocks.3.ffn.key.weight\n",
      "2560  2560  0    blocks.3.ffn.receptance.weight\n",
      "2560  10240 0    blocks.3.ffn.value.weight\n",
      "2560  2560  0    blocks.4.att.key.weight\n",
      "2560  2560  1.0  blocks.4.att.value.weight\n",
      "2560  2560  0    blocks.4.att.receptance.weight\n",
      "2560  2560  0    blocks.4.att.output.weight\n",
      "10240 2560  1.0  blocks.4.ffn.key.weight\n",
      "2560  2560  0    blocks.4.ffn.receptance.weight\n",
      "2560  10240 0    blocks.4.ffn.value.weight\n",
      "2560  2560  0    blocks.5.att.key.weight\n",
      "2560  2560  1.0  blocks.5.att.value.weight\n",
      "2560  2560  0    blocks.5.att.receptance.weight\n",
      "2560  2560  0    blocks.5.att.output.weight\n",
      "10240 2560  1.0  blocks.5.ffn.key.weight\n",
      "2560  2560  0    blocks.5.ffn.receptance.weight\n",
      "2560  10240 0    blocks.5.ffn.value.weight\n",
      "2560  2560  0    blocks.6.att.key.weight\n",
      "2560  2560  1.0  blocks.6.att.value.weight\n",
      "2560  2560  0    blocks.6.att.receptance.weight\n",
      "2560  2560  0    blocks.6.att.output.weight\n",
      "10240 2560  1.0  blocks.6.ffn.key.weight\n",
      "2560  2560  0    blocks.6.ffn.receptance.weight\n",
      "2560  10240 0    blocks.6.ffn.value.weight\n",
      "2560  2560  0    blocks.7.att.key.weight\n",
      "2560  2560  1.0  blocks.7.att.value.weight\n",
      "2560  2560  0    blocks.7.att.receptance.weight\n",
      "2560  2560  0    blocks.7.att.output.weight\n",
      "10240 2560  1.0  blocks.7.ffn.key.weight\n",
      "2560  2560  0    blocks.7.ffn.receptance.weight\n",
      "2560  10240 0    blocks.7.ffn.value.weight\n",
      "2560  2560  0    blocks.8.att.key.weight\n",
      "2560  2560  1.0  blocks.8.att.value.weight\n",
      "2560  2560  0    blocks.8.att.receptance.weight\n",
      "2560  2560  0    blocks.8.att.output.weight\n",
      "10240 2560  1.0  blocks.8.ffn.key.weight\n",
      "2560  2560  0    blocks.8.ffn.receptance.weight\n",
      "2560  10240 0    blocks.8.ffn.value.weight\n",
      "2560  2560  0    blocks.9.att.key.weight\n",
      "2560  2560  1.0  blocks.9.att.value.weight\n",
      "2560  2560  0    blocks.9.att.receptance.weight\n",
      "2560  2560  0    blocks.9.att.output.weight\n",
      "10240 2560  1.0  blocks.9.ffn.key.weight\n",
      "2560  2560  0    blocks.9.ffn.receptance.weight\n",
      "2560  10240 0    blocks.9.ffn.value.weight\n",
      "2560  2560  0    blocks.10.att.key.weight\n",
      "2560  2560  1.0  blocks.10.att.value.weight\n",
      "2560  2560  0    blocks.10.att.receptance.weight\n",
      "2560  2560  0    blocks.10.att.output.weight\n",
      "10240 2560  1.0  blocks.10.ffn.key.weight\n",
      "2560  2560  0    blocks.10.ffn.receptance.weight\n",
      "2560  10240 0    blocks.10.ffn.value.weight\n",
      "2560  2560  0    blocks.11.att.key.weight\n",
      "2560  2560  1.0  blocks.11.att.value.weight\n",
      "2560  2560  0    blocks.11.att.receptance.weight\n",
      "2560  2560  0    blocks.11.att.output.weight\n",
      "10240 2560  1.0  blocks.11.ffn.key.weight\n",
      "2560  2560  0    blocks.11.ffn.receptance.weight\n",
      "2560  10240 0    blocks.11.ffn.value.weight\n",
      "2560  2560  0    blocks.12.att.key.weight\n",
      "2560  2560  1.0  blocks.12.att.value.weight\n",
      "2560  2560  0    blocks.12.att.receptance.weight\n",
      "2560  2560  0    blocks.12.att.output.weight\n",
      "10240 2560  1.0  blocks.12.ffn.key.weight\n",
      "2560  2560  0    blocks.12.ffn.receptance.weight\n",
      "2560  10240 0    blocks.12.ffn.value.weight\n",
      "2560  2560  0    blocks.13.att.key.weight\n",
      "2560  2560  1.0  blocks.13.att.value.weight\n",
      "2560  2560  0    blocks.13.att.receptance.weight\n",
      "2560  2560  0    blocks.13.att.output.weight\n",
      "10240 2560  1.0  blocks.13.ffn.key.weight\n",
      "2560  2560  0    blocks.13.ffn.receptance.weight\n",
      "2560  10240 0    blocks.13.ffn.value.weight\n",
      "2560  2560  0    blocks.14.att.key.weight\n",
      "2560  2560  1.0  blocks.14.att.value.weight\n",
      "2560  2560  0    blocks.14.att.receptance.weight\n",
      "2560  2560  0    blocks.14.att.output.weight\n",
      "10240 2560  1.0  blocks.14.ffn.key.weight\n",
      "2560  2560  0    blocks.14.ffn.receptance.weight\n",
      "2560  10240 0    blocks.14.ffn.value.weight\n",
      "2560  2560  0    blocks.15.att.key.weight\n",
      "2560  2560  1.0  blocks.15.att.value.weight\n",
      "2560  2560  0    blocks.15.att.receptance.weight\n",
      "2560  2560  0    blocks.15.att.output.weight\n",
      "10240 2560  1.0  blocks.15.ffn.key.weight\n",
      "2560  2560  0    blocks.15.ffn.receptance.weight\n",
      "2560  10240 0    blocks.15.ffn.value.weight\n",
      "2560  2560  0    blocks.16.att.key.weight\n",
      "2560  2560  1.0  blocks.16.att.value.weight\n",
      "2560  2560  0    blocks.16.att.receptance.weight\n",
      "2560  2560  0    blocks.16.att.output.weight\n",
      "10240 2560  1.0  blocks.16.ffn.key.weight\n",
      "2560  2560  0    blocks.16.ffn.receptance.weight\n",
      "2560  10240 0    blocks.16.ffn.value.weight\n",
      "2560  2560  0    blocks.17.att.key.weight\n",
      "2560  2560  1.0  blocks.17.att.value.weight\n",
      "2560  2560  0    blocks.17.att.receptance.weight\n",
      "2560  2560  0    blocks.17.att.output.weight\n",
      "10240 2560  1.0  blocks.17.ffn.key.weight\n",
      "2560  2560  0    blocks.17.ffn.receptance.weight\n",
      "2560  10240 0    blocks.17.ffn.value.weight\n",
      "2560  2560  0    blocks.18.att.key.weight\n",
      "2560  2560  1.0  blocks.18.att.value.weight\n",
      "2560  2560  0    blocks.18.att.receptance.weight\n",
      "2560  2560  0    blocks.18.att.output.weight\n",
      "10240 2560  1.0  blocks.18.ffn.key.weight\n",
      "2560  2560  0    blocks.18.ffn.receptance.weight\n",
      "2560  10240 0    blocks.18.ffn.value.weight\n",
      "2560  2560  0    blocks.19.att.key.weight\n",
      "2560  2560  1.0  blocks.19.att.value.weight\n",
      "2560  2560  0    blocks.19.att.receptance.weight\n",
      "2560  2560  0    blocks.19.att.output.weight\n",
      "10240 2560  1.0  blocks.19.ffn.key.weight\n",
      "2560  2560  0    blocks.19.ffn.receptance.weight\n",
      "2560  10240 0    blocks.19.ffn.value.weight\n",
      "2560  2560  0    blocks.20.att.key.weight\n",
      "2560  2560  1.0  blocks.20.att.value.weight\n",
      "2560  2560  0    blocks.20.att.receptance.weight\n",
      "2560  2560  0    blocks.20.att.output.weight\n",
      "10240 2560  1.0  blocks.20.ffn.key.weight\n",
      "2560  2560  0    blocks.20.ffn.receptance.weight\n",
      "2560  10240 0    blocks.20.ffn.value.weight\n",
      "2560  2560  0    blocks.21.att.key.weight\n",
      "2560  2560  1.0  blocks.21.att.value.weight\n",
      "2560  2560  0    blocks.21.att.receptance.weight\n",
      "2560  2560  0    blocks.21.att.output.weight\n",
      "10240 2560  1.0  blocks.21.ffn.key.weight\n",
      "2560  2560  0    blocks.21.ffn.receptance.weight\n",
      "2560  10240 0    blocks.21.ffn.value.weight\n",
      "2560  2560  0    blocks.22.att.key.weight\n",
      "2560  2560  1.0  blocks.22.att.value.weight\n",
      "2560  2560  0    blocks.22.att.receptance.weight\n",
      "2560  2560  0    blocks.22.att.output.weight\n",
      "10240 2560  1.0  blocks.22.ffn.key.weight\n",
      "2560  2560  0    blocks.22.ffn.receptance.weight\n",
      "2560  10240 0    blocks.22.ffn.value.weight\n",
      "2560  2560  0    blocks.23.att.key.weight\n",
      "2560  2560  1.0  blocks.23.att.value.weight\n",
      "2560  2560  0    blocks.23.att.receptance.weight\n",
      "2560  2560  0    blocks.23.att.output.weight\n",
      "10240 2560  1.0  blocks.23.ffn.key.weight\n",
      "2560  2560  0    blocks.23.ffn.receptance.weight\n",
      "2560  10240 0    blocks.23.ffn.value.weight\n",
      "2560  2560  0    blocks.24.att.key.weight\n",
      "2560  2560  1.0  blocks.24.att.value.weight\n",
      "2560  2560  0    blocks.24.att.receptance.weight\n",
      "2560  2560  0    blocks.24.att.output.weight\n",
      "10240 2560  1.0  blocks.24.ffn.key.weight\n",
      "2560  2560  0    blocks.24.ffn.receptance.weight\n",
      "2560  10240 0    blocks.24.ffn.value.weight\n",
      "2560  2560  0    blocks.25.att.key.weight\n",
      "2560  2560  1.0  blocks.25.att.value.weight\n",
      "2560  2560  0    blocks.25.att.receptance.weight\n",
      "2560  2560  0    blocks.25.att.output.weight\n",
      "10240 2560  1.0  blocks.25.ffn.key.weight\n",
      "2560  2560  0    blocks.25.ffn.receptance.weight\n",
      "2560  10240 0    blocks.25.ffn.value.weight\n",
      "2560  2560  0    blocks.26.att.key.weight\n",
      "2560  2560  1.0  blocks.26.att.value.weight\n",
      "2560  2560  0    blocks.26.att.receptance.weight\n",
      "2560  2560  0    blocks.26.att.output.weight\n",
      "10240 2560  1.0  blocks.26.ffn.key.weight\n",
      "2560  2560  0    blocks.26.ffn.receptance.weight\n",
      "2560  10240 0    blocks.26.ffn.value.weight\n",
      "2560  2560  0    blocks.27.att.key.weight\n",
      "2560  2560  1.0  blocks.27.att.value.weight\n",
      "2560  2560  0    blocks.27.att.receptance.weight\n",
      "2560  2560  0    blocks.27.att.output.weight\n",
      "10240 2560  1.0  blocks.27.ffn.key.weight\n",
      "2560  2560  0    blocks.27.ffn.receptance.weight\n",
      "2560  10240 0    blocks.27.ffn.value.weight\n",
      "2560  2560  0    blocks.28.att.key.weight\n",
      "2560  2560  1.0  blocks.28.att.value.weight\n",
      "2560  2560  0    blocks.28.att.receptance.weight\n",
      "2560  2560  0    blocks.28.att.output.weight\n",
      "10240 2560  1.0  blocks.28.ffn.key.weight\n",
      "2560  2560  0    blocks.28.ffn.receptance.weight\n",
      "2560  10240 0    blocks.28.ffn.value.weight\n",
      "2560  2560  0    blocks.29.att.key.weight\n",
      "2560  2560  1.0  blocks.29.att.value.weight\n",
      "2560  2560  0    blocks.29.att.receptance.weight\n",
      "2560  2560  0    blocks.29.att.output.weight\n",
      "10240 2560  1.0  blocks.29.ffn.key.weight\n",
      "2560  2560  0    blocks.29.ffn.receptance.weight\n",
      "2560  10240 0    blocks.29.ffn.value.weight\n",
      "2560  2560  0    blocks.30.att.key.weight\n",
      "2560  2560  1.0  blocks.30.att.value.weight\n",
      "2560  2560  0    blocks.30.att.receptance.weight\n",
      "2560  2560  0    blocks.30.att.output.weight\n",
      "10240 2560  1.0  blocks.30.ffn.key.weight\n",
      "2560  2560  0    blocks.30.ffn.receptance.weight\n",
      "2560  10240 0    blocks.30.ffn.value.weight\n",
      "2560  2560  0    blocks.31.att.key.weight\n",
      "2560  2560  1.0  blocks.31.att.value.weight\n",
      "2560  2560  0    blocks.31.att.receptance.weight\n",
      "2560  2560  0    blocks.31.att.output.weight\n",
      "10240 2560  1.0  blocks.31.ffn.key.weight\n",
      "2560  2560  0    blocks.31.ffn.receptance.weight\n",
      "2560  10240 0    blocks.31.ffn.value.weight\n",
      "50277 2560  0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 init_model.py \\\n",
    "        --n_layer 32 --n_embd 2560 \\\n",
    "        --vocab_size neox \\\n",
    "        ../model/L32-D2560-neox-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2615317921\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2615317921\n",
      "[RWKV.model]: Preloading model from '../model/L32-D2560-neox-init.pth'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[RWKV.model]: Loading model weights ( L32-D2560-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 789.00it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-1062ba7da82e617a_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-49681c8aebfc1cc9_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-47774371f91aea93_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 2615317921                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-08-01 15:00:24,928] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 2615317921\n",
      "[RWKV.model]: Preloading model from '../model/L32-D2560-neox-init.pth'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[RWKV.model]: Loading model weights ( L32-D2560-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "[rank: 1] Global seed set to 2615317921\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-08-01 15:00:54,136] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[RWKV.model][rank=1] Configuring optimizer ...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.4066073894500732 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.4224231243133545 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.08424139022827148 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10212993621826172 seconds\n",
      "Rank: 0 partition count [2, 2, 2] and sizes[(1492231680, False), (40960, False), (40960, False)] \n",
      "Rank: 1 partition count [2, 2, 2] and sizes[(1492231680, False), (40960, False), (40960, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003390312194824219 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028014183044433594 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 128 M \n",
      "1 | blocks | ModuleList | 2.7 B \n",
      "2 | ln_out | LayerNorm  | 5.1 K \n",
      "3 | head   | Linear     | 128 M \n",
      "--------------------------------------\n",
      "3.0 B     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 B     Total params\n",
      "11,938.509Total estimated model params size (MB)\n",
      "Epoch 0:   0%|   | 10/2648 [01:45<7:45:39, 10.59s/it, v_num=2, train/loss=9.000]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_steps=10` reached.\n",
      "Epoch 0:   0%|  | 10/2648 [03:12<14:05:49, 19.24s/it, v_num=2, train/loss=9.000]\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/minimal-enwiki.yaml\" \\\n",
    "        --model.load_model \"../model/L32-D2560-neox-init.pth\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7B Size (L32-D4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "---- Initializing model ----\n",
      "No of layers: 32\n",
      "Embedding size: 4096\n",
      "Output model path: ../model/L32-D4096-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_2048_bf16/build.ninja...\n",
      "Building extension module wkv_2048_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_2048_bf16...\n",
      "[RWKV.model]: Finished initial model load\n",
      "50277 4096  -0.1 emb.weight\n",
      "4096  4096  0    blocks.0.att.key.weight\n",
      "4096  4096  1.0  blocks.0.att.value.weight\n",
      "4096  4096  0    blocks.0.att.receptance.weight\n",
      "4096  4096  0    blocks.0.att.output.weight\n",
      "16384 4096  1.0  blocks.0.ffn.key.weight\n",
      "4096  4096  0    blocks.0.ffn.receptance.weight\n",
      "4096  16384 0    blocks.0.ffn.value.weight\n",
      "4096  4096  0    blocks.1.att.key.weight\n",
      "4096  4096  1.0  blocks.1.att.value.weight\n",
      "4096  4096  0    blocks.1.att.receptance.weight\n",
      "4096  4096  0    blocks.1.att.output.weight\n",
      "16384 4096  1.0  blocks.1.ffn.key.weight\n",
      "4096  4096  0    blocks.1.ffn.receptance.weight\n",
      "4096  16384 0    blocks.1.ffn.value.weight\n",
      "4096  4096  0    blocks.2.att.key.weight\n",
      "4096  4096  1.0  blocks.2.att.value.weight\n",
      "4096  4096  0    blocks.2.att.receptance.weight\n",
      "4096  4096  0    blocks.2.att.output.weight\n",
      "16384 4096  1.0  blocks.2.ffn.key.weight\n",
      "4096  4096  0    blocks.2.ffn.receptance.weight\n",
      "4096  16384 0    blocks.2.ffn.value.weight\n",
      "4096  4096  0    blocks.3.att.key.weight\n",
      "4096  4096  1.0  blocks.3.att.value.weight\n",
      "4096  4096  0    blocks.3.att.receptance.weight\n",
      "4096  4096  0    blocks.3.att.output.weight\n",
      "16384 4096  1.0  blocks.3.ffn.key.weight\n",
      "4096  4096  0    blocks.3.ffn.receptance.weight\n",
      "4096  16384 0    blocks.3.ffn.value.weight\n",
      "4096  4096  0    blocks.4.att.key.weight\n",
      "4096  4096  1.0  blocks.4.att.value.weight\n",
      "4096  4096  0    blocks.4.att.receptance.weight\n",
      "4096  4096  0    blocks.4.att.output.weight\n",
      "16384 4096  1.0  blocks.4.ffn.key.weight\n",
      "4096  4096  0    blocks.4.ffn.receptance.weight\n",
      "4096  16384 0    blocks.4.ffn.value.weight\n",
      "4096  4096  0    blocks.5.att.key.weight\n",
      "4096  4096  1.0  blocks.5.att.value.weight\n",
      "4096  4096  0    blocks.5.att.receptance.weight\n",
      "4096  4096  0    blocks.5.att.output.weight\n",
      "16384 4096  1.0  blocks.5.ffn.key.weight\n",
      "4096  4096  0    blocks.5.ffn.receptance.weight\n",
      "4096  16384 0    blocks.5.ffn.value.weight\n",
      "4096  4096  0    blocks.6.att.key.weight\n",
      "4096  4096  1.0  blocks.6.att.value.weight\n",
      "4096  4096  0    blocks.6.att.receptance.weight\n",
      "4096  4096  0    blocks.6.att.output.weight\n",
      "16384 4096  1.0  blocks.6.ffn.key.weight\n",
      "4096  4096  0    blocks.6.ffn.receptance.weight\n",
      "4096  16384 0    blocks.6.ffn.value.weight\n",
      "4096  4096  0    blocks.7.att.key.weight\n",
      "4096  4096  1.0  blocks.7.att.value.weight\n",
      "4096  4096  0    blocks.7.att.receptance.weight\n",
      "4096  4096  0    blocks.7.att.output.weight\n",
      "16384 4096  1.0  blocks.7.ffn.key.weight\n",
      "4096  4096  0    blocks.7.ffn.receptance.weight\n",
      "4096  16384 0    blocks.7.ffn.value.weight\n",
      "4096  4096  0    blocks.8.att.key.weight\n",
      "4096  4096  1.0  blocks.8.att.value.weight\n",
      "4096  4096  0    blocks.8.att.receptance.weight\n",
      "4096  4096  0    blocks.8.att.output.weight\n",
      "16384 4096  1.0  blocks.8.ffn.key.weight\n",
      "4096  4096  0    blocks.8.ffn.receptance.weight\n",
      "4096  16384 0    blocks.8.ffn.value.weight\n",
      "4096  4096  0    blocks.9.att.key.weight\n",
      "4096  4096  1.0  blocks.9.att.value.weight\n",
      "4096  4096  0    blocks.9.att.receptance.weight\n",
      "4096  4096  0    blocks.9.att.output.weight\n",
      "16384 4096  1.0  blocks.9.ffn.key.weight\n",
      "4096  4096  0    blocks.9.ffn.receptance.weight\n",
      "4096  16384 0    blocks.9.ffn.value.weight\n",
      "4096  4096  0    blocks.10.att.key.weight\n",
      "4096  4096  1.0  blocks.10.att.value.weight\n",
      "4096  4096  0    blocks.10.att.receptance.weight\n",
      "4096  4096  0    blocks.10.att.output.weight\n",
      "16384 4096  1.0  blocks.10.ffn.key.weight\n",
      "4096  4096  0    blocks.10.ffn.receptance.weight\n",
      "4096  16384 0    blocks.10.ffn.value.weight\n",
      "4096  4096  0    blocks.11.att.key.weight\n",
      "4096  4096  1.0  blocks.11.att.value.weight\n",
      "4096  4096  0    blocks.11.att.receptance.weight\n",
      "4096  4096  0    blocks.11.att.output.weight\n",
      "16384 4096  1.0  blocks.11.ffn.key.weight\n",
      "4096  4096  0    blocks.11.ffn.receptance.weight\n",
      "4096  16384 0    blocks.11.ffn.value.weight\n",
      "4096  4096  0    blocks.12.att.key.weight\n",
      "4096  4096  1.0  blocks.12.att.value.weight\n",
      "4096  4096  0    blocks.12.att.receptance.weight\n",
      "4096  4096  0    blocks.12.att.output.weight\n",
      "16384 4096  1.0  blocks.12.ffn.key.weight\n",
      "4096  4096  0    blocks.12.ffn.receptance.weight\n",
      "4096  16384 0    blocks.12.ffn.value.weight\n",
      "4096  4096  0    blocks.13.att.key.weight\n",
      "4096  4096  1.0  blocks.13.att.value.weight\n",
      "4096  4096  0    blocks.13.att.receptance.weight\n",
      "4096  4096  0    blocks.13.att.output.weight\n",
      "16384 4096  1.0  blocks.13.ffn.key.weight\n",
      "4096  4096  0    blocks.13.ffn.receptance.weight\n",
      "4096  16384 0    blocks.13.ffn.value.weight\n",
      "4096  4096  0    blocks.14.att.key.weight\n",
      "4096  4096  1.0  blocks.14.att.value.weight\n",
      "4096  4096  0    blocks.14.att.receptance.weight\n",
      "4096  4096  0    blocks.14.att.output.weight\n",
      "16384 4096  1.0  blocks.14.ffn.key.weight\n",
      "4096  4096  0    blocks.14.ffn.receptance.weight\n",
      "4096  16384 0    blocks.14.ffn.value.weight\n",
      "4096  4096  0    blocks.15.att.key.weight\n",
      "4096  4096  1.0  blocks.15.att.value.weight\n",
      "4096  4096  0    blocks.15.att.receptance.weight\n",
      "4096  4096  0    blocks.15.att.output.weight\n",
      "16384 4096  1.0  blocks.15.ffn.key.weight\n",
      "4096  4096  0    blocks.15.ffn.receptance.weight\n",
      "4096  16384 0    blocks.15.ffn.value.weight\n",
      "4096  4096  0    blocks.16.att.key.weight\n",
      "4096  4096  1.0  blocks.16.att.value.weight\n",
      "4096  4096  0    blocks.16.att.receptance.weight\n",
      "4096  4096  0    blocks.16.att.output.weight\n",
      "16384 4096  1.0  blocks.16.ffn.key.weight\n",
      "4096  4096  0    blocks.16.ffn.receptance.weight\n",
      "4096  16384 0    blocks.16.ffn.value.weight\n",
      "4096  4096  0    blocks.17.att.key.weight\n",
      "4096  4096  1.0  blocks.17.att.value.weight\n",
      "4096  4096  0    blocks.17.att.receptance.weight\n",
      "4096  4096  0    blocks.17.att.output.weight\n",
      "16384 4096  1.0  blocks.17.ffn.key.weight\n",
      "4096  4096  0    blocks.17.ffn.receptance.weight\n",
      "4096  16384 0    blocks.17.ffn.value.weight\n",
      "4096  4096  0    blocks.18.att.key.weight\n",
      "4096  4096  1.0  blocks.18.att.value.weight\n",
      "4096  4096  0    blocks.18.att.receptance.weight\n",
      "4096  4096  0    blocks.18.att.output.weight\n",
      "16384 4096  1.0  blocks.18.ffn.key.weight\n",
      "4096  4096  0    blocks.18.ffn.receptance.weight\n",
      "4096  16384 0    blocks.18.ffn.value.weight\n",
      "4096  4096  0    blocks.19.att.key.weight\n",
      "4096  4096  1.0  blocks.19.att.value.weight\n",
      "4096  4096  0    blocks.19.att.receptance.weight\n",
      "4096  4096  0    blocks.19.att.output.weight\n",
      "16384 4096  1.0  blocks.19.ffn.key.weight\n",
      "4096  4096  0    blocks.19.ffn.receptance.weight\n",
      "4096  16384 0    blocks.19.ffn.value.weight\n",
      "4096  4096  0    blocks.20.att.key.weight\n",
      "4096  4096  1.0  blocks.20.att.value.weight\n",
      "4096  4096  0    blocks.20.att.receptance.weight\n",
      "4096  4096  0    blocks.20.att.output.weight\n",
      "16384 4096  1.0  blocks.20.ffn.key.weight\n",
      "4096  4096  0    blocks.20.ffn.receptance.weight\n",
      "4096  16384 0    blocks.20.ffn.value.weight\n",
      "4096  4096  0    blocks.21.att.key.weight\n",
      "4096  4096  1.0  blocks.21.att.value.weight\n",
      "4096  4096  0    blocks.21.att.receptance.weight\n",
      "4096  4096  0    blocks.21.att.output.weight\n",
      "16384 4096  1.0  blocks.21.ffn.key.weight\n",
      "4096  4096  0    blocks.21.ffn.receptance.weight\n",
      "4096  16384 0    blocks.21.ffn.value.weight\n",
      "4096  4096  0    blocks.22.att.key.weight\n",
      "4096  4096  1.0  blocks.22.att.value.weight\n",
      "4096  4096  0    blocks.22.att.receptance.weight\n",
      "4096  4096  0    blocks.22.att.output.weight\n",
      "16384 4096  1.0  blocks.22.ffn.key.weight\n",
      "4096  4096  0    blocks.22.ffn.receptance.weight\n",
      "4096  16384 0    blocks.22.ffn.value.weight\n",
      "4096  4096  0    blocks.23.att.key.weight\n",
      "4096  4096  1.0  blocks.23.att.value.weight\n",
      "4096  4096  0    blocks.23.att.receptance.weight\n",
      "4096  4096  0    blocks.23.att.output.weight\n",
      "16384 4096  1.0  blocks.23.ffn.key.weight\n",
      "4096  4096  0    blocks.23.ffn.receptance.weight\n",
      "4096  16384 0    blocks.23.ffn.value.weight\n",
      "4096  4096  0    blocks.24.att.key.weight\n",
      "4096  4096  1.0  blocks.24.att.value.weight\n",
      "4096  4096  0    blocks.24.att.receptance.weight\n",
      "4096  4096  0    blocks.24.att.output.weight\n",
      "16384 4096  1.0  blocks.24.ffn.key.weight\n",
      "4096  4096  0    blocks.24.ffn.receptance.weight\n",
      "4096  16384 0    blocks.24.ffn.value.weight\n",
      "4096  4096  0    blocks.25.att.key.weight\n",
      "4096  4096  1.0  blocks.25.att.value.weight\n",
      "4096  4096  0    blocks.25.att.receptance.weight\n",
      "4096  4096  0    blocks.25.att.output.weight\n",
      "16384 4096  1.0  blocks.25.ffn.key.weight\n",
      "4096  4096  0    blocks.25.ffn.receptance.weight\n",
      "4096  16384 0    blocks.25.ffn.value.weight\n",
      "4096  4096  0    blocks.26.att.key.weight\n",
      "4096  4096  1.0  blocks.26.att.value.weight\n",
      "4096  4096  0    blocks.26.att.receptance.weight\n",
      "4096  4096  0    blocks.26.att.output.weight\n",
      "16384 4096  1.0  blocks.26.ffn.key.weight\n",
      "4096  4096  0    blocks.26.ffn.receptance.weight\n",
      "4096  16384 0    blocks.26.ffn.value.weight\n",
      "4096  4096  0    blocks.27.att.key.weight\n",
      "4096  4096  1.0  blocks.27.att.value.weight\n",
      "4096  4096  0    blocks.27.att.receptance.weight\n",
      "4096  4096  0    blocks.27.att.output.weight\n",
      "16384 4096  1.0  blocks.27.ffn.key.weight\n",
      "4096  4096  0    blocks.27.ffn.receptance.weight\n",
      "4096  16384 0    blocks.27.ffn.value.weight\n",
      "4096  4096  0    blocks.28.att.key.weight\n",
      "4096  4096  1.0  blocks.28.att.value.weight\n",
      "4096  4096  0    blocks.28.att.receptance.weight\n",
      "4096  4096  0    blocks.28.att.output.weight\n",
      "16384 4096  1.0  blocks.28.ffn.key.weight\n",
      "4096  4096  0    blocks.28.ffn.receptance.weight\n",
      "4096  16384 0    blocks.28.ffn.value.weight\n",
      "4096  4096  0    blocks.29.att.key.weight\n",
      "4096  4096  1.0  blocks.29.att.value.weight\n",
      "4096  4096  0    blocks.29.att.receptance.weight\n",
      "4096  4096  0    blocks.29.att.output.weight\n",
      "16384 4096  1.0  blocks.29.ffn.key.weight\n",
      "4096  4096  0    blocks.29.ffn.receptance.weight\n",
      "4096  16384 0    blocks.29.ffn.value.weight\n",
      "4096  4096  0    blocks.30.att.key.weight\n",
      "4096  4096  1.0  blocks.30.att.value.weight\n",
      "4096  4096  0    blocks.30.att.receptance.weight\n",
      "4096  4096  0    blocks.30.att.output.weight\n",
      "16384 4096  1.0  blocks.30.ffn.key.weight\n",
      "4096  4096  0    blocks.30.ffn.receptance.weight\n",
      "4096  16384 0    blocks.30.ffn.value.weight\n",
      "4096  4096  0    blocks.31.att.key.weight\n",
      "4096  4096  1.0  blocks.31.att.value.weight\n",
      "4096  4096  0    blocks.31.att.receptance.weight\n",
      "4096  4096  0    blocks.31.att.output.weight\n",
      "16384 4096  1.0  blocks.31.ffn.key.weight\n",
      "4096  4096  0    blocks.31.ffn.receptance.weight\n",
      "4096  16384 0    blocks.31.ffn.value.weight\n",
      "50277 4096  0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 init_model.py \\\n",
    "        --n_layer 32 --n_embd 4096 \\\n",
    "        --vocab_size neox \\\n",
    "        ../model/L32-D4096-neox-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1719644420\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1719644420\n",
      "[RWKV.model]: Preloading model from '../model/L32-D4096-neox-init.pth'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[RWKV.model]: Loading model weights ( L32-D4096-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 136.98it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-1062ba7da82e617a_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-49681c8aebfc1cc9_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-47774371f91aea93_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 1719644420                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-08-01 15:11:41,862] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 1719644420\n",
      "[RWKV.model]: Preloading model from '../model/L32-D4096-neox-init.pth'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[RWKV.model]: Loading model weights ( L32-D4096-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "[rank: 1] Global seed set to 1719644420\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-08-01 15:12:46,908] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[RWKV.model][rank=1] Configuring optimizer ...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.4030609130859375 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.420283317565918 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.11437869071960449 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.20235228538513184 seconds\n",
      "Rank: 0 partition count [2, 2, 2] and sizes[(3696193536, False), (65536, False), (65536, False)] \n",
      "Rank: 1 partition count [2, 2, 2] and sizes[(3696193536, False), (65536, False), (65536, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0320279598236084 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0023527145385742188 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 205 M \n",
      "1 | blocks | ModuleList | 7.0 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 205 M \n",
      "--------------------------------------\n",
      "7.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "7.4 B     Total params\n",
      "29,570.597Total estimated model params size (MB)\n",
      "Epoch 0:   0%|  | 10/2648 [04:44<20:51:48, 28.47s/it, v_num=3, train/loss=9.500]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_steps=10` reached.\n",
      "Epoch 0:   0%|  | 10/2648 [05:44<25:16:31, 34.49s/it, v_num=3, train/loss=9.500]\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/minimal-enwiki.yaml\" \\\n",
    "        --model.load_model \"../model/L32-D4096-neox-init.pth\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14B Size (L40-D5120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "---- Initializing model ----\n",
      "No of layers: 40\n",
      "Embedding size: 5120\n",
      "Output model path: ../model/L40-D5120-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_2048_bf16/build.ninja...\n",
      "Building extension module wkv_2048_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_2048_bf16...\n",
      "[RWKV.model]: Finished initial model load\n",
      "50277 5120  -0.1 emb.weight\n",
      "5120  5120  0    blocks.0.att.key.weight\n",
      "5120  5120  1.0  blocks.0.att.value.weight\n",
      "5120  5120  0    blocks.0.att.receptance.weight\n",
      "5120  5120  0    blocks.0.att.output.weight\n",
      "20480 5120  1.0  blocks.0.ffn.key.weight\n",
      "5120  5120  0    blocks.0.ffn.receptance.weight\n",
      "5120  20480 0    blocks.0.ffn.value.weight\n",
      "5120  5120  0    blocks.1.att.key.weight\n",
      "5120  5120  1.0  blocks.1.att.value.weight\n",
      "5120  5120  0    blocks.1.att.receptance.weight\n",
      "5120  5120  0    blocks.1.att.output.weight\n",
      "20480 5120  1.0  blocks.1.ffn.key.weight\n",
      "5120  5120  0    blocks.1.ffn.receptance.weight\n",
      "5120  20480 0    blocks.1.ffn.value.weight\n",
      "5120  5120  0    blocks.2.att.key.weight\n",
      "5120  5120  1.0  blocks.2.att.value.weight\n",
      "5120  5120  0    blocks.2.att.receptance.weight\n",
      "5120  5120  0    blocks.2.att.output.weight\n",
      "20480 5120  1.0  blocks.2.ffn.key.weight\n",
      "5120  5120  0    blocks.2.ffn.receptance.weight\n",
      "5120  20480 0    blocks.2.ffn.value.weight\n",
      "5120  5120  0    blocks.3.att.key.weight\n",
      "5120  5120  1.0  blocks.3.att.value.weight\n",
      "5120  5120  0    blocks.3.att.receptance.weight\n",
      "5120  5120  0    blocks.3.att.output.weight\n",
      "20480 5120  1.0  blocks.3.ffn.key.weight\n",
      "5120  5120  0    blocks.3.ffn.receptance.weight\n",
      "5120  20480 0    blocks.3.ffn.value.weight\n",
      "5120  5120  0    blocks.4.att.key.weight\n",
      "5120  5120  1.0  blocks.4.att.value.weight\n",
      "5120  5120  0    blocks.4.att.receptance.weight\n",
      "5120  5120  0    blocks.4.att.output.weight\n",
      "20480 5120  1.0  blocks.4.ffn.key.weight\n",
      "5120  5120  0    blocks.4.ffn.receptance.weight\n",
      "5120  20480 0    blocks.4.ffn.value.weight\n",
      "5120  5120  0    blocks.5.att.key.weight\n",
      "5120  5120  1.0  blocks.5.att.value.weight\n",
      "5120  5120  0    blocks.5.att.receptance.weight\n",
      "5120  5120  0    blocks.5.att.output.weight\n",
      "20480 5120  1.0  blocks.5.ffn.key.weight\n",
      "5120  5120  0    blocks.5.ffn.receptance.weight\n",
      "5120  20480 0    blocks.5.ffn.value.weight\n",
      "5120  5120  0    blocks.6.att.key.weight\n",
      "5120  5120  1.0  blocks.6.att.value.weight\n",
      "5120  5120  0    blocks.6.att.receptance.weight\n",
      "5120  5120  0    blocks.6.att.output.weight\n",
      "20480 5120  1.0  blocks.6.ffn.key.weight\n",
      "5120  5120  0    blocks.6.ffn.receptance.weight\n",
      "5120  20480 0    blocks.6.ffn.value.weight\n",
      "5120  5120  0    blocks.7.att.key.weight\n",
      "5120  5120  1.0  blocks.7.att.value.weight\n",
      "5120  5120  0    blocks.7.att.receptance.weight\n",
      "5120  5120  0    blocks.7.att.output.weight\n",
      "20480 5120  1.0  blocks.7.ffn.key.weight\n",
      "5120  5120  0    blocks.7.ffn.receptance.weight\n",
      "5120  20480 0    blocks.7.ffn.value.weight\n",
      "5120  5120  0    blocks.8.att.key.weight\n",
      "5120  5120  1.0  blocks.8.att.value.weight\n",
      "5120  5120  0    blocks.8.att.receptance.weight\n",
      "5120  5120  0    blocks.8.att.output.weight\n",
      "20480 5120  1.0  blocks.8.ffn.key.weight\n",
      "5120  5120  0    blocks.8.ffn.receptance.weight\n",
      "5120  20480 0    blocks.8.ffn.value.weight\n",
      "5120  5120  0    blocks.9.att.key.weight\n",
      "5120  5120  1.0  blocks.9.att.value.weight\n",
      "5120  5120  0    blocks.9.att.receptance.weight\n",
      "5120  5120  0    blocks.9.att.output.weight\n",
      "20480 5120  1.0  blocks.9.ffn.key.weight\n",
      "5120  5120  0    blocks.9.ffn.receptance.weight\n",
      "5120  20480 0    blocks.9.ffn.value.weight\n",
      "5120  5120  0    blocks.10.att.key.weight\n",
      "5120  5120  1.0  blocks.10.att.value.weight\n",
      "5120  5120  0    blocks.10.att.receptance.weight\n",
      "5120  5120  0    blocks.10.att.output.weight\n",
      "20480 5120  1.0  blocks.10.ffn.key.weight\n",
      "5120  5120  0    blocks.10.ffn.receptance.weight\n",
      "5120  20480 0    blocks.10.ffn.value.weight\n",
      "5120  5120  0    blocks.11.att.key.weight\n",
      "5120  5120  1.0  blocks.11.att.value.weight\n",
      "5120  5120  0    blocks.11.att.receptance.weight\n",
      "5120  5120  0    blocks.11.att.output.weight\n",
      "20480 5120  1.0  blocks.11.ffn.key.weight\n",
      "5120  5120  0    blocks.11.ffn.receptance.weight\n",
      "5120  20480 0    blocks.11.ffn.value.weight\n",
      "5120  5120  0    blocks.12.att.key.weight\n",
      "5120  5120  1.0  blocks.12.att.value.weight\n",
      "5120  5120  0    blocks.12.att.receptance.weight\n",
      "5120  5120  0    blocks.12.att.output.weight\n",
      "20480 5120  1.0  blocks.12.ffn.key.weight\n",
      "5120  5120  0    blocks.12.ffn.receptance.weight\n",
      "5120  20480 0    blocks.12.ffn.value.weight\n",
      "5120  5120  0    blocks.13.att.key.weight\n",
      "5120  5120  1.0  blocks.13.att.value.weight\n",
      "5120  5120  0    blocks.13.att.receptance.weight\n",
      "5120  5120  0    blocks.13.att.output.weight\n",
      "20480 5120  1.0  blocks.13.ffn.key.weight\n",
      "5120  5120  0    blocks.13.ffn.receptance.weight\n",
      "5120  20480 0    blocks.13.ffn.value.weight\n",
      "5120  5120  0    blocks.14.att.key.weight\n",
      "5120  5120  1.0  blocks.14.att.value.weight\n",
      "5120  5120  0    blocks.14.att.receptance.weight\n",
      "5120  5120  0    blocks.14.att.output.weight\n",
      "20480 5120  1.0  blocks.14.ffn.key.weight\n",
      "5120  5120  0    blocks.14.ffn.receptance.weight\n",
      "5120  20480 0    blocks.14.ffn.value.weight\n",
      "5120  5120  0    blocks.15.att.key.weight\n",
      "5120  5120  1.0  blocks.15.att.value.weight\n",
      "5120  5120  0    blocks.15.att.receptance.weight\n",
      "5120  5120  0    blocks.15.att.output.weight\n",
      "20480 5120  1.0  blocks.15.ffn.key.weight\n",
      "5120  5120  0    blocks.15.ffn.receptance.weight\n",
      "5120  20480 0    blocks.15.ffn.value.weight\n",
      "5120  5120  0    blocks.16.att.key.weight\n",
      "5120  5120  1.0  blocks.16.att.value.weight\n",
      "5120  5120  0    blocks.16.att.receptance.weight\n",
      "5120  5120  0    blocks.16.att.output.weight\n",
      "20480 5120  1.0  blocks.16.ffn.key.weight\n",
      "5120  5120  0    blocks.16.ffn.receptance.weight\n",
      "5120  20480 0    blocks.16.ffn.value.weight\n",
      "5120  5120  0    blocks.17.att.key.weight\n",
      "5120  5120  1.0  blocks.17.att.value.weight\n",
      "5120  5120  0    blocks.17.att.receptance.weight\n",
      "5120  5120  0    blocks.17.att.output.weight\n",
      "20480 5120  1.0  blocks.17.ffn.key.weight\n",
      "5120  5120  0    blocks.17.ffn.receptance.weight\n",
      "5120  20480 0    blocks.17.ffn.value.weight\n",
      "5120  5120  0    blocks.18.att.key.weight\n",
      "5120  5120  1.0  blocks.18.att.value.weight\n",
      "5120  5120  0    blocks.18.att.receptance.weight\n",
      "5120  5120  0    blocks.18.att.output.weight\n",
      "20480 5120  1.0  blocks.18.ffn.key.weight\n",
      "5120  5120  0    blocks.18.ffn.receptance.weight\n",
      "5120  20480 0    blocks.18.ffn.value.weight\n",
      "5120  5120  0    blocks.19.att.key.weight\n",
      "5120  5120  1.0  blocks.19.att.value.weight\n",
      "5120  5120  0    blocks.19.att.receptance.weight\n",
      "5120  5120  0    blocks.19.att.output.weight\n",
      "20480 5120  1.0  blocks.19.ffn.key.weight\n",
      "5120  5120  0    blocks.19.ffn.receptance.weight\n",
      "5120  20480 0    blocks.19.ffn.value.weight\n",
      "5120  5120  0    blocks.20.att.key.weight\n",
      "5120  5120  1.0  blocks.20.att.value.weight\n",
      "5120  5120  0    blocks.20.att.receptance.weight\n",
      "5120  5120  0    blocks.20.att.output.weight\n",
      "20480 5120  1.0  blocks.20.ffn.key.weight\n",
      "5120  5120  0    blocks.20.ffn.receptance.weight\n",
      "5120  20480 0    blocks.20.ffn.value.weight\n",
      "5120  5120  0    blocks.21.att.key.weight\n",
      "5120  5120  1.0  blocks.21.att.value.weight\n",
      "5120  5120  0    blocks.21.att.receptance.weight\n",
      "5120  5120  0    blocks.21.att.output.weight\n",
      "20480 5120  1.0  blocks.21.ffn.key.weight\n",
      "5120  5120  0    blocks.21.ffn.receptance.weight\n",
      "5120  20480 0    blocks.21.ffn.value.weight\n",
      "5120  5120  0    blocks.22.att.key.weight\n",
      "5120  5120  1.0  blocks.22.att.value.weight\n",
      "5120  5120  0    blocks.22.att.receptance.weight\n",
      "5120  5120  0    blocks.22.att.output.weight\n",
      "20480 5120  1.0  blocks.22.ffn.key.weight\n",
      "5120  5120  0    blocks.22.ffn.receptance.weight\n",
      "5120  20480 0    blocks.22.ffn.value.weight\n",
      "5120  5120  0    blocks.23.att.key.weight\n",
      "5120  5120  1.0  blocks.23.att.value.weight\n",
      "5120  5120  0    blocks.23.att.receptance.weight\n",
      "5120  5120  0    blocks.23.att.output.weight\n",
      "20480 5120  1.0  blocks.23.ffn.key.weight\n",
      "5120  5120  0    blocks.23.ffn.receptance.weight\n",
      "5120  20480 0    blocks.23.ffn.value.weight\n",
      "5120  5120  0    blocks.24.att.key.weight\n",
      "5120  5120  1.0  blocks.24.att.value.weight\n",
      "5120  5120  0    blocks.24.att.receptance.weight\n",
      "5120  5120  0    blocks.24.att.output.weight\n",
      "20480 5120  1.0  blocks.24.ffn.key.weight\n",
      "5120  5120  0    blocks.24.ffn.receptance.weight\n",
      "5120  20480 0    blocks.24.ffn.value.weight\n",
      "5120  5120  0    blocks.25.att.key.weight\n",
      "5120  5120  1.0  blocks.25.att.value.weight\n",
      "5120  5120  0    blocks.25.att.receptance.weight\n",
      "5120  5120  0    blocks.25.att.output.weight\n",
      "20480 5120  1.0  blocks.25.ffn.key.weight\n",
      "5120  5120  0    blocks.25.ffn.receptance.weight\n",
      "5120  20480 0    blocks.25.ffn.value.weight\n",
      "5120  5120  0    blocks.26.att.key.weight\n",
      "5120  5120  1.0  blocks.26.att.value.weight\n",
      "5120  5120  0    blocks.26.att.receptance.weight\n",
      "5120  5120  0    blocks.26.att.output.weight\n",
      "20480 5120  1.0  blocks.26.ffn.key.weight\n",
      "5120  5120  0    blocks.26.ffn.receptance.weight\n",
      "5120  20480 0    blocks.26.ffn.value.weight\n",
      "5120  5120  0    blocks.27.att.key.weight\n",
      "5120  5120  1.0  blocks.27.att.value.weight\n",
      "5120  5120  0    blocks.27.att.receptance.weight\n",
      "5120  5120  0    blocks.27.att.output.weight\n",
      "20480 5120  1.0  blocks.27.ffn.key.weight\n",
      "5120  5120  0    blocks.27.ffn.receptance.weight\n",
      "5120  20480 0    blocks.27.ffn.value.weight\n",
      "5120  5120  0    blocks.28.att.key.weight\n",
      "5120  5120  1.0  blocks.28.att.value.weight\n",
      "5120  5120  0    blocks.28.att.receptance.weight\n",
      "5120  5120  0    blocks.28.att.output.weight\n",
      "20480 5120  1.0  blocks.28.ffn.key.weight\n",
      "5120  5120  0    blocks.28.ffn.receptance.weight\n",
      "5120  20480 0    blocks.28.ffn.value.weight\n",
      "5120  5120  0    blocks.29.att.key.weight\n",
      "5120  5120  1.0  blocks.29.att.value.weight\n",
      "5120  5120  0    blocks.29.att.receptance.weight\n",
      "5120  5120  0    blocks.29.att.output.weight\n",
      "20480 5120  1.0  blocks.29.ffn.key.weight\n",
      "5120  5120  0    blocks.29.ffn.receptance.weight\n",
      "5120  20480 0    blocks.29.ffn.value.weight\n",
      "5120  5120  0    blocks.30.att.key.weight\n",
      "5120  5120  1.0  blocks.30.att.value.weight\n",
      "5120  5120  0    blocks.30.att.receptance.weight\n",
      "5120  5120  0    blocks.30.att.output.weight\n",
      "20480 5120  1.0  blocks.30.ffn.key.weight\n",
      "5120  5120  0    blocks.30.ffn.receptance.weight\n",
      "5120  20480 0    blocks.30.ffn.value.weight\n",
      "5120  5120  0    blocks.31.att.key.weight\n",
      "5120  5120  1.0  blocks.31.att.value.weight\n",
      "5120  5120  0    blocks.31.att.receptance.weight\n",
      "5120  5120  0    blocks.31.att.output.weight\n",
      "20480 5120  1.0  blocks.31.ffn.key.weight\n",
      "5120  5120  0    blocks.31.ffn.receptance.weight\n",
      "5120  20480 0    blocks.31.ffn.value.weight\n",
      "5120  5120  0    blocks.32.att.key.weight\n",
      "5120  5120  1.0  blocks.32.att.value.weight\n",
      "5120  5120  0    blocks.32.att.receptance.weight\n",
      "5120  5120  0    blocks.32.att.output.weight\n",
      "20480 5120  1.0  blocks.32.ffn.key.weight\n",
      "5120  5120  0    blocks.32.ffn.receptance.weight\n",
      "5120  20480 0    blocks.32.ffn.value.weight\n",
      "5120  5120  0    blocks.33.att.key.weight\n",
      "5120  5120  1.0  blocks.33.att.value.weight\n",
      "5120  5120  0    blocks.33.att.receptance.weight\n",
      "5120  5120  0    blocks.33.att.output.weight\n",
      "20480 5120  1.0  blocks.33.ffn.key.weight\n",
      "5120  5120  0    blocks.33.ffn.receptance.weight\n",
      "5120  20480 0    blocks.33.ffn.value.weight\n",
      "5120  5120  0    blocks.34.att.key.weight\n",
      "5120  5120  1.0  blocks.34.att.value.weight\n",
      "5120  5120  0    blocks.34.att.receptance.weight\n",
      "5120  5120  0    blocks.34.att.output.weight\n",
      "20480 5120  1.0  blocks.34.ffn.key.weight\n",
      "5120  5120  0    blocks.34.ffn.receptance.weight\n",
      "5120  20480 0    blocks.34.ffn.value.weight\n",
      "5120  5120  0    blocks.35.att.key.weight\n",
      "5120  5120  1.0  blocks.35.att.value.weight\n",
      "5120  5120  0    blocks.35.att.receptance.weight\n",
      "5120  5120  0    blocks.35.att.output.weight\n",
      "20480 5120  1.0  blocks.35.ffn.key.weight\n",
      "5120  5120  0    blocks.35.ffn.receptance.weight\n",
      "5120  20480 0    blocks.35.ffn.value.weight\n",
      "5120  5120  0    blocks.36.att.key.weight\n",
      "5120  5120  1.0  blocks.36.att.value.weight\n",
      "5120  5120  0    blocks.36.att.receptance.weight\n",
      "5120  5120  0    blocks.36.att.output.weight\n",
      "20480 5120  1.0  blocks.36.ffn.key.weight\n",
      "5120  5120  0    blocks.36.ffn.receptance.weight\n",
      "5120  20480 0    blocks.36.ffn.value.weight\n",
      "5120  5120  0    blocks.37.att.key.weight\n",
      "5120  5120  1.0  blocks.37.att.value.weight\n",
      "5120  5120  0    blocks.37.att.receptance.weight\n",
      "5120  5120  0    blocks.37.att.output.weight\n",
      "20480 5120  1.0  blocks.37.ffn.key.weight\n",
      "5120  5120  0    blocks.37.ffn.receptance.weight\n",
      "5120  20480 0    blocks.37.ffn.value.weight\n",
      "5120  5120  0    blocks.38.att.key.weight\n",
      "5120  5120  1.0  blocks.38.att.value.weight\n",
      "5120  5120  0    blocks.38.att.receptance.weight\n",
      "5120  5120  0    blocks.38.att.output.weight\n",
      "20480 5120  1.0  blocks.38.ffn.key.weight\n",
      "5120  5120  0    blocks.38.ffn.receptance.weight\n",
      "5120  20480 0    blocks.38.ffn.value.weight\n",
      "5120  5120  0    blocks.39.att.key.weight\n",
      "5120  5120  1.0  blocks.39.att.value.weight\n",
      "5120  5120  0    blocks.39.att.receptance.weight\n",
      "5120  5120  0    blocks.39.att.output.weight\n",
      "20480 5120  1.0  blocks.39.ffn.key.weight\n",
      "5120  5120  0    blocks.39.ffn.receptance.weight\n",
      "5120  20480 0    blocks.39.ffn.value.weight\n",
      "50277 5120  0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 init_model.py \\\n",
    "        --n_layer 40 --n_embd 5120 \\\n",
    "        --vocab_size neox \\\n",
    "        ../model/L40-D5120-neox-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3, disabling JIT using RWKV_JIT_ON=0\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2913082349\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2913082349\n",
      "[RWKV.model]: Preloading model from '../model/L40-D5120-neox-init.pth'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[RWKV.model]: Loading model weights ( L40-D5120-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 49.57it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-1062ba7da82e617a_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-49681c8aebfc1cc9_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-47774371f91aea93_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 2913082349                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-08-01 17:04:05,048] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3, disabling JIT using RWKV_JIT_ON=0\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 2913082349\n",
      "[RWKV.model]: Preloading model from '../model/L40-D5120-neox-init.pth'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[RWKV.model]: Loading model weights ( L40-D5120-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "[rank: 1] Global seed set to 2913082349\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-08-01 17:06:02,703] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[RWKV.model][rank=1] Configuring optimizer ...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.08968353271484375 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10188531875610352 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.11159062385559082 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.08856415748596191 seconds\n",
      "Parameter Offload: Total persistent parameters: 2273280 in 444 params\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/picocreator-dev-infctx/RWKV-v4neo/lightning_trainer.py\", line 74, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/picocreator-dev-infctx/RWKV-v4neo/lightning_trainer.py\", line 59, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 911, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 344, in setup\n",
      "    self.init_deepspeed()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 448, in init_deepspeed\n",
      "    self._initialize_deepspeed_train(model)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 484, in _initialize_deepspeed_train\n",
      "    model, deepspeed_optimizer = self._setup_model_and_optimizer(model, optimizer, scheduler)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 413, in _setup_model_and_optimizer\n",
      "    deepspeed_engine, deepspeed_optimizer, _, _ = deepspeed.initialize(\n",
      "                                                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/__init__.py\", line 165, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 309, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1185, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1475, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/zero/stage3.py\", line 304, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/zero/stage3.py\", line 378, in _setup_for_real_optimizer\n",
      "    self._create_fp32_partitions()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/zero/stage3.py\", line 752, in _create_fp32_partitions\n",
      "    self.device).clone().float().detach())\n",
      "                         ^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.35 GiB (GPU 1; 47.54 GiB total capacity; 26.35 GiB already allocated; 20.76 GiB free; 26.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/picocreator-dev-infctx/RWKV-v4neo/lightning_trainer.py\", line 74, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/picocreator-dev-infctx/RWKV-v4neo/lightning_trainer.py\", line 59, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 911, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 344, in setup\n",
      "    self.init_deepspeed()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 448, in init_deepspeed\n",
      "    self._initialize_deepspeed_train(model)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 484, in _initialize_deepspeed_train\n",
      "    model, deepspeed_optimizer = self._setup_model_and_optimizer(model, optimizer, scheduler)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 413, in _setup_model_and_optimizer\n",
      "    deepspeed_engine, deepspeed_optimizer, _, _ = deepspeed.initialize(\n",
      "                                                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/__init__.py\", line 165, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 309, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1185, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1475, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/zero/stage3.py\", line 304, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/zero/stage3.py\", line 378, in _setup_for_real_optimizer\n",
      "    self._create_fp32_partitions()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/zero/stage3.py\", line 752, in _create_fp32_partitions\n",
      "    self.device).clone().float().detach())\n",
      "                         ^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.35 GiB (GPU 0; 47.54 GiB total capacity; 26.35 GiB already allocated; 20.76 GiB free; 26.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/minimal-enwiki.yaml\" \\\n",
    "        --model.load_model \"../model/L40-D5120-neox-init.pth\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
